<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 宋军亮的技术博客</title>
    <link>/posts/</link>
    <description>Recent content in Posts on 宋军亮的技术博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>pywin32控制你的打印机</title>
      <link>/2019/%E9%87%87%E7%94%A8pywin32%E6%8E%A7%E5%88%B6%E4%BD%A0%E7%9A%84%E6%89%93%E5%8D%B0%E6%9C%BA/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/%E9%87%87%E7%94%A8pywin32%E6%8E%A7%E5%88%B6%E4%BD%A0%E7%9A%84%E6%89%93%E5%8D%B0%E6%9C%BA/</guid>
      <description>pywin23控制打印机</description>
    </item>
    
    <item>
      <title>bs4</title>
      <link>/2019/python-beautifulsoup4/</link>
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/python-beautifulsoup4/</guid>
      <description>Beautiful Soup 是一个可以从 HTML 或 XML 文件中提取数据的 Python 库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup 会帮你节省数小时甚至数天的工作时间.
 安装 以下都是在 python2.7 中进行测试的  可以直接使用 pip 安装：
$ pip install beautifulsoup4 BeautifulSoup 不仅支持 HTML 解析器,还支持一些第三方的解析器，如，lxml，XML，html5lib 但是需要安装相应的库。
$ pip install lxml
$ pip install html5lib 2. 开始使用 Beautiful Soup 的功能相当强大，但我们只介绍经常使用的功能。
简单用法 将一段文档传入 BeautifulSoup 的构造方法,就能得到一个文档的对象, 可以传入一段字符串或一个文件句柄.
   from bs4 import BeautifulSoup
soup = BeautifulSoup(&amp;rdquo;data
&amp;rdquo;)
soup data

soup(&amp;lsquo;p&amp;rsquo;) [data
] 首先传入一个 html 文档，soup 是获得文档的对象。然后,文档被转换成 Unicode ,并且 HTML 的实例都被转换成 Unicode 编码。然后,Beautiful Soup 选择最合适的解析器来解析这段文档,如果手动指定解析器那么 Beautiful Soup 会选择指定的解析器来解析文档。但是一般最好手动指定解析器，并且使用 requests 与 BeautifulSoup 结合使用， requests 是用于爬取网页源码的一个库，此处不再介绍，requests 更多用法请参考 Requests 2.</description>
    </item>
    
    <item>
      <title>单链表</title>
      <link>/2019/%E5%8D%95%E9%93%BE%E8%A1%A8/</link>
      <pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/%E5%8D%95%E9%93%BE%E8%A1%A8/</guid>
      <description>单向链表也叫单链表，是链表中最简单的一种形式，它的每个节点包含两个域，一个信息域（元素域）和一个链接域。这个链接指向链表中的下一个节点，而最后一个节点的链接域则指向一个空值。 1.表元素域elem用来存放具体的数据。 2.链接域next用来存放下一个节点的位置 3.变量p指向链表的头节点（首节点）的位置，从p出发能找到表中的任意节点。
class SingleNode(object): &amp;quot;&amp;quot;&amp;quot;单链表的结点&amp;quot;&amp;quot;&amp;quot; def __init__(self,item): # _item存放数据元素 self.item = item # _next是下一个节点的标识 self.next = None  单链表的操作
is_empty() 链表是否为空 length() 链表长度 travel() 遍历整个链表 add(item) 链表头部添加元素 append(item) 链表尾部添加元素 insert(pos, item) 指定位置添加元素 remove(item) 删除节点 search(item) 查找节点是否存在
class SingleLinkList(object): &amp;quot;&amp;quot;&amp;quot;单链表&amp;quot;&amp;quot;&amp;quot; def __init__(self): self._head = None def is_empty(self): &amp;quot;&amp;quot;&amp;quot;判断链表是否为空&amp;quot;&amp;quot;&amp;quot; return self._head == None def length(self): &amp;quot;&amp;quot;&amp;quot;链表长度&amp;quot;&amp;quot;&amp;quot; # cur初始时指向头节点 cur = self._head count = 0 # 尾节点指向None，当未到达尾部时 while cur != None: count += 1 # 将cur后移一个节点 cur = cur.</description>
    </item>
    
    <item>
      <title>python链表操作</title>
      <link>/2019/python%E9%93%BE%E8%A1%A8%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/python%E9%93%BE%E8%A1%A8%E6%93%8D%E4%BD%9C/</guid>
      <description>顺序表的构建需要预先知道数据大小来申请连续的存储空间，而在进行扩充时又需要进行数据的搬迁，所以使用起来并不是很灵活。链表结构可以充分利用计算机内存空间，实现灵活的内存动态管理。
链表的定义： 链表（Linked list）是一种常见的基础数据结构，是一种线性表，但是不像顺序表一样连续存储数据，而是在每一个节点（数据存储单元）里存放下一个节点的位置信息（即地址）。
链表失去了顺序表随机读取的优点，同时链表由于增加了结点的指针域，空间开销比较大，但对存储空间的使用要相对灵活。 注意： 链表和顺序表在插入和删除时进行的是完全不同的操作。链表的主要耗时操作是遍历查找，删除和插入操作本身的复杂度是O(1)。顺序表查找很快，主要耗时的操作是拷贝覆盖。因为除了目标元素在尾部的特殊情况，顺序表进行插入和删除时需要对操作点之后的所有元素进行前后移位操作，只能通过拷贝和覆盖的方法进行。
1、创建一个链接 node1 = Node(&amp;ldquo;c&amp;rdquo;,node3) 或者 node1 = Node(&amp;ldquo;c&amp;rdquo;,None) node1.next = node3 2、用循环创建一个链表结构，并且访问其中的每一个节点 class Node(object): def init(self, data, next=None): self.data = data self.next = next
head = None for count in range(1,6): head = Node(count, head) while head != None: print(head.data) head = head.next 3、遍历 遍历使用一个临时的指针变量，这个变量先初始化为链表结构的head指针，然后控制一个循环。
probe = head while probe != None: probe = probe.next 4、搜索 有两个终止条件：
 一、空链表，不再有要检查的数据。
 二、目标项等于数据项，成功找到。
probe = head while probe !</description>
    </item>
    
    <item>
      <title>单链表反转</title>
      <link>/2019/%E5%8D%95%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/%E5%8D%95%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/</guid>
      <description>单链表结构 head &amp;mdash; node1 &amp;mdash; node2 &amp;mdash; node3 &amp;mdash; node4 &amp;mdash;- None
反转实现 建立三个变量，L、M、R互相赋值迭代，并建立指向关系，从而实现单链表的反转。 python代码实现 class Node(object): def init(self, data, next=None): self.val = data self.next = next
def fun4(head): if head == None: return None L,M,R = None,None,head while R.next != None: L = M M = R R = R.next M.next = L R.next = M return R #测试用例 if name == &amp;lsquo;main&amp;lsquo;: l1 = Node(3) l1.next = Node(2) l1.</description>
    </item>
    
    <item>
      <title>聚焦爬虫和通用爬虫</title>
      <link>/2019/%E8%81%9A%E7%84%A6%E7%88%AC%E8%99%AB%E5%92%8C%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/%E8%81%9A%E7%84%A6%E7%88%AC%E8%99%AB%E5%92%8C%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB/</guid>
      <description>1.通用爬虫：搜索引擎用的爬虫系统。搜索引擎和供应商提供的爬虫。
　通用爬虫要遵循规则：Robots协议 通用爬虫工作流程： 爬取网页》存储数据》内容处理》提供检索 通用爬虫缺点： 只能提供和文本相关的内容如html、world、pdf等，不能提供多媒体文件如音乐、图片、视频和二进制文件（脚本、程序） 提供的结果千篇一律，针对不同领域提供不同内容 不能提供人类语义上的检索 通用爬虫局限性： 1.通用搜索引擎返回网页的数据内容，大概90%都无用。 2.中文搜索引擎自然语言检索理解困难。 3.信息占有量和覆盖率存在局限。 4.搜索引擎主要是以关键字搜索为主，对于图片、数据库、视频、音频等多媒体的内容用通用搜索引擎无效。 5.搜索引擎的社区化和个性化不好，未考虑实际因素如人的地域、性别、年龄等差别。 6.搜索引擎爬取动态网页效果不好
2.聚焦爬虫：针对于某一需求编写的爬虫程序。 聚焦爬虫可分为三类： 1.积累式爬虫：从开始到结束，不断爬取，过程会进行重复操作。
　2.增量爬虫：已下载网页采取增量式跟新，爬取更新变化的数据。 3.深度爬虫：指那些不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获取的web界面。</description>
    </item>
    
    <item>
      <title>多线程cpu/gil</title>
      <link>/2019/%E5%A4%9A%E7%BA%BF%E7%A8%8Bcpu%E4%B8%8Egil/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/%E5%A4%9A%E7%BA%BF%E7%A8%8Bcpu%E4%B8%8Egil/</guid>
      <description>多线程与CPU： 1.单核CPU CPU密集型的程序（做计算操作的程序） 单线程即可（ 此时的任务已经把CPU资源100%消耗了，就没必要也不可能使用多线程来提高计算效率） 2.单核CPU IO密集型的程序（做IO操作的程序 ） 多线程&amp;gt;单线程（多线程可以阻塞，但并不是并行，是“伪并行”，实际上还是一个CPU在执行一切事物，只是切换的太快，没法察觉） 3.多核CPU 做计算操作的程序 多线程&amp;gt;&amp;gt;单线程 （每个核心执行一个线程，每个核心的线程并发执行计算，以提高任务执行效率，例如加密解密，数据压缩解压缩（视频、音频、普通数据），否则只能使一个核心满载，而其他核心闲置。）
4.多核CPU IO密集型任务 多线程&amp;gt;单线程
但是在PYTHON里：
由于GIL的机制就变得不完全一样了：单核CPU CPU密集型程序 单线程耗时&amp;lt;多线程。多核CPU CPU密集型程序 单线程耗时&amp;lt;多线程，也就是说只要是CPU密集型程序 不要只单独的使用多线程。
一、先说为什么会有GIL ，GIL是干什么的：
多线程之间数据完整性和状态同步的最简单方法自然就是加锁，于是python解释器有了GIL这把超级大锁，就默认python内部对象是thread-safe的，无需在实现时考虑额外的内存锁和同步操作。
也就是说 如果不释放这把锁，线程都是串行的。
但PYTHON的多线程并不是一无是处
二、GIL锁的释放机制：
先解释什么是IO密集型和计算密集型：
计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。
计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。
第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。
IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。
GIL对于IO密集型和计算密集型：
IO密集型（网络传输、磁盘读写等）： 线程遇到I/O阻塞时，会自动释放GIL。（阻塞等待时，就释放GIL，给另一个线程执行的机会）
cpu密集型（编解码，解压缩等）： 解释器会周期性的让线程释放锁
由上面可知，至少有两种情况python会做线程切换，一是一但有IO操作时，会有线程切换，二是当一个线程连续执行了一定数量的指令时，会出现线程切换。
再加上每次操作系统执行线程的调度都需要消耗时间，这样，就可以理解为什么在多核+cpu密集型程序时不要单独的使用多线程，会比单线程更耗时。
即使是多核CPU，如果没有GIL 不同核的CPU执行不同的线程，但是有了GIL这把锁，某个核心上的CPU即使被唤醒也没有获得GIL锁，无法执行。
在不使用别的库的情况下，python多线程最好只用于IO密集型的操作.</description>
    </item>
    
    <item>
      <title>数据库优化</title>
      <link>/2019/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96/</link>
      <pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96/</guid>
      <description>一：优化说明 A：有数据表明，用户可以承受的最大等待时间为8秒。数据库优化策略有很多，设计初期，建立好的数据结构对于后期性能优化至关重要。因为数据库结构是系统的基石，基础打不好，使用各种优化策略，也不能达到很完美的效果。
B：数据库优化的几个方面
可以看出来，数据结构、SQL、索引是成本最低，且效果最好的优化手段。
C：性能优化是无止境的，当性能可以满足需求时即可，不要过度优化。
二：优化方向 1. SQL以及索引的优化 首先要根据需求写出结构良好的SQL，然后根据SQL在表中建立有效的索引。但是如果索引太多，不但会影响写入的效率，对查询也有一定的影响。
 合理的数据库是设计 根据数据库三范式来进行表结构的设计。设计表结构时，就需要考虑如何设计才能更有效的查询。  数据库三范式： 第一范式：数据表中每个字段都必须是不可拆分的最小单元，也就是确保每一列的原子性； 第二范式：满足一范式后，表中每一列必须有唯一性，都必须依赖于主键； 第三范式：满足二范式后，表中的每一列只与主键直接相关而不是间接相关(外键也是直接相关)，字段没有冗余。
注意：没有最好的设计，只有最合适的设计，所以不要过分注重理论。三范式可以作为一个基本依据，不要生搬硬套。
有时候可以根据场景合理地反规范化： A：分割表。 B：保留冗余字段。当两个或多个表在查询中经常需要连接时，可以在其中一个表上增加若干冗余的字段，以 避免表之间的连接过于频繁，一般在冗余列的数据不经常变动的情况下使用。 C：增加派生列。派生列是由表中的其它多个列的计算所得，增加派生列可以减少统计运算，在数据汇总时可以大大缩短运算时间。
数据库五大约束： A：PRIMARY key:设置主键约束； B：UNIQUE：设置唯一性约束，不能有重复值； C：DEFAULT 默认值约束 D：NOT NULL：设置非空约束，该字段不能为空； E：FOREIGN key :设置外键约束。
字段类型选择： A：尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED B：VARCHAR的长度只分配真正需要的空间 C：使用枚举或整数代替字符串类型 D：尽量使用TIMESTAMP而非DATETIME E：单表不要有太多字段，建议在20以内 F：避免使用NULL字段，很难查询优化且占用额外索引空间
 系统配置的优化 例如：MySQL数据库my.cnf
 硬件优化 更快的IO、更多的内存。一般来说内存越大，对于数据库的操作越好。但是CPU多就不一定了，因为他并不会用到太多的CPU数量，有很多的查询都是单CPU。另外使用高的IO（SSD、RAID），但是IO并不能减少数据库锁的机制。所以说如果查询缓慢是因为数据库内部的一些锁引起的，那么硬件优化就没有什么意义。
  三：优化方案 代码优化 之所以把代码放到第一位，是因为这一点最容易引起技术人员的忽视。很多技术人员拿到一个性能优化的需求以后，言必称缓存、异步、JVM等。实际上，第一步就应该是分析相关的代码，找出相应的瓶颈，再来考虑具体的优化策略。有一些性能问题，完全是由于代码写的不合理，通过直接修改一下代码就能解决问题的，比如for循环次数过多、作了很多无谓的条件判断、相同逻辑重复多次等。
举个栗子： 一个update操作，先查询出entity，再执行update，这样无疑多了一次数据库交互。还有一个问题，update语句可能会操作一些无需更新的字段。 ​ 我们可以将表单中涉及到的属性，以及updateTime，updateUser等赋值到entity，直接通过pdateByPrimaryKeySelective，去update特定字段。
​​
定位慢SQL，并优化 这是最常用、每一个技术人员都应该掌握基本的SQL调优手段（包括方法、工具、辅助系统等）。这里以MySQL为例，最常见的方式是，由自带的慢查询日志或者开源的慢查询系统定位到具体的出问题的SQL，然后使用explain、profile等工具来逐步调优，最后经过测试达到效果后上线。
SqlServer执行计划： 通过执行计划，我们能得到哪些信息： A：哪些步骤花费的成本比较高 B：哪些步骤产生的数据量多，数据量的多少用线条的粗细表示，很直观 C：每一步执行了什么动作
具体优化手段： A：尽量少用（或者不用）sqlserver 自带的函数 select id from t where substring(name,1,3) = ’abc’ select id from t where datediff(day,createdate,’2005-11-30′) = 0 可以这样查询： select id from t where name like ‘abc%’ select id from t where createdate &amp;gt;= ‘2005-11-30’ and createdate &amp;lt; ‘2005-12-1’</description>
    </item>
    
    <item>
      <title>数据库sql优化</title>
      <link>/2019/%E6%95%B0%E6%8D%AE%E5%BA%93sql%E4%BC%98%E5%8C%96/</link>
      <pubDate>Mon, 22 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/%E6%95%B0%E6%8D%AE%E5%BA%93sql%E4%BC%98%E5%8C%96/</guid>
      <description>1.对查询进行优化，要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。
2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：
select id from t where num is null  最好不要给数据库留NULL，尽可能的使用 NOT NULL填充数据库.
备注、描述、评论之类的可以设置为 NULL，其他的，最好不要使用NULL。
不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL也包含在内），都是占用 100个字符的空间的，如果是varchar这样的变长字段， null 不占用空间。
可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：
select id from t where num = 0  3.应尽量避免在 where 子句中使用 != 或 &amp;lt;&amp;gt; 操作符，否则将引擎放弃使用索引而进行全表扫描。
4.应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描，如：
select id from t where num=10 or Name = &#39;admin&#39; 可以这样查询： select id from t where num = 10 union all select id from t where Name = &#39;admin&#39;  5.</description>
    </item>
    
    <item>
      <title>爬虫逆向工程</title>
      <link>/2019/%E7%88%AC%E8%99%AB%E9%80%86%E5%90%91%E5%B7%A5%E7%A8%8B/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/%E7%88%AC%E8%99%AB%E9%80%86%E5%90%91%E5%B7%A5%E7%A8%8B/</guid>
      <description>爬虫篇：动态网页的处理方式（上）——逆向工程  https://blog.csdn.net/ha_hha/article/details/80324343
2.简谈-Python爬虫破解JS加密的Cookie
https://www.cnblogs.com/zccpython/p/6886634.html
3.运用phantomjs无头浏览器破解四种反爬虫技术
http://python.jobbole.com/86415/
4.Python爬虫教程-16-破解js加密实例（有道在线翻译）
https://blog.csdn.net/qq_40147863/article/details/82079649</description>
    </item>
    
    <item>
      <title>python面试题收藏</title>
      <link>/2019/python%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/python%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>git上的面试题 https://github.com/kenwoodjw/python_interview_question https://github.com/taizilongxu/interview_python
一些简单的Python测试题 http://www.wklken.me/posts/2015/08/26/python-some-test-questions.html https://baijiahao.baidu.com/s?id=1607651363840614527&amp;amp;wfr=spider&amp;amp;for=pc
面试Python后端的技巧 https://blog.csdn.net/yueguanghaidao/article/details/49638261
一个python面试经历 https://mp.weixin.qq.com/s?__biz=MzU0ODczMTEwOQ==&amp;amp;mid=2247486666&amp;amp;idx=1&amp;amp;sn=709653e9cb60ca65ea6636bf7cdd8381&amp;amp;source=41#wechat_redirect</description>
    </item>
    
    <item>
      <title>Nginx的启动与安装</title>
      <link>/2019/nginx%E7%9A%84%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%AE%89%E8%A3%85/</link>
      <pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/nginx%E7%9A%84%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%AE%89%E8%A3%85/</guid>
      <description>启动 启动代码格式：nginx安装目录地址 -c nginx配置文件地址
例如：
[root@LinuxServer sbin]# /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf
停止 nginx的停止有三种方式：
 ## 从容停止  　1、查看进程号
[root@LinuxServer ~]# ps -ef|grep nginx
　2、杀死进程
[root@LinuxServer ~]# kill -QUIT 2072
 ## 快速停止  1、查看进程号
[root@LinuxServer ~]# ps -ef|grep nginx
2、杀死进程
[root@LinuxServer ~]# kill -TERM 2132 或 [root@LinuxServer ~]# kill -INT 2132
- ## 强制停止
[root@LinuxServer ~]# pkill -9 nginx
重启 1、验证nginx配置文件是否正确 方法一：进入nginx安装目录sbin下，输入命令./nginx -t 看到如下显示nginx.conf syntax is ok
nginx.conf test is successful</description>
    </item>
    
    <item>
      <title>Mysql的一些报错</title>
      <link>/2019/mysql%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%A5%E9%94%99/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/mysql%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%A5%E9%94%99/</guid>
      <description>MySQL 连接错误Can&amp;rsquo;t connect to MySQL server on (61) 链接数据库时忽然遇到一个问题。Mac Navicat链接时报错Can’t connect to MySQL server on ‘xx.xx.xx.xx’ (61)。
PS. win版Navicat 报错Can’t connect to MySQL server on ‘xx.xx.xx.xx’ (10038)
其中xx.xx.xx.xx是ip地址。
1、查看该用户是否有远程登录的权限 ===
mysql&amp;gt; SELECT * FROM mysql.user; +-----------+-----------+ | User | Host | +-----------+-----------+ | M | % | | mysql.sys | localhost | | root | localhost | | tommy | ％ | | showhilllee | ％ | +-----------+-----------+ 5 rows in set (0.</description>
    </item>
    
    <item>
      <title>scrapy是什么</title>
      <link>/2018/scrapy%E7%9A%84%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/scrapy%E7%9A%84%E4%BD%BF%E7%94%A8/</guid>
      <description>Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。
Scrapy 常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。
通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。
Scrapy架构图(绿线是数据流向)
Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。
Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。
Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，
Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器).
Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。
Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。
Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）</description>
    </item>
    
    <item>
      <title>正向路由与普通路由</title>
      <link>/2018/%E6%AD%A3%E5%90%91%E8%B7%AF%E7%94%B1%E4%B8%8E%E6%99%AE%E9%80%9A%E8%B7%AF%E7%94%B1/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/%E6%AD%A3%E5%90%91%E8%B7%AF%E7%94%B1%E4%B8%8E%E6%99%AE%E9%80%9A%E8%B7%AF%E7%94%B1/</guid>
      <description>什么是路由？ ​ URL是Web服务的入口，用户通过浏览器发送过来的任何请求，都是发送到一个指定的URL地址，然后被响应。
在项目中编写路由，就是向外暴露我们接收哪些URL的请求，除此之外的任何URL都不被处理，也没有返回。通俗地理解，不恰当的形容，URL路由是你的Web服务对外暴露的API。
django使用反向解析需要添加应用的命名空间，例如上图的 app_name = “booktest” booktest是你应用的名字 使用反向解析需要在url匹配后添加自己定义的name
反向解析传递参数空格就行，这里的2就表示需要传递的参数，booktest就是你当前的这个应用的名字，fortest2就是你需要之前定义的名字
什么是正则路由？ 在开发过程中，可能会出现限制用户访问规则的场景，那么这个时候就需要用到正则匹配，根据自己的规则去限定请求参数再进行访问。
例子：
 url(r&amp;rsquo;^index&amp;rsquo;,views.index) #默认的 url(r&amp;rsquo;^home&amp;rsquo;,views.Home.as_view()) # CBV 形式 也就是说 class
 url(r&amp;rsquo;^detail-(\d+).html&amp;rsquo;, views.detail), # 在views里 def home(request, nid, uid) 这两个参数不能调换循序
  ### # http: //www.baidu.com/detail-2-8.html 2是nid 8是uid
 url (r&amp;rsquo;^detail-(?P&amp;lt; nid&amp;gt;\d+)-(?P&amp;lt; uid&amp;gt;\d+) .html&amp;rsquo;, views.detail) #多个正则 在views里 def home(request, nid, uid) 这两个参数可以没有顺序  什么情况下使用反向解析？ 所谓反向解析就是根据命名来调到指定的页面，这里表示的是跳转到booktest模板下面的fortest2这个视图里面， 而这个fortest2正是我们定义的名字，后面的2表示需要传递过去的参数
一、urls硬编码 在反向解析和命名空间之前我们先来说说URLS硬编码，用django 开发应用的时候，可以完全是在urls.py 中硬编码配置地址,在views.py中HttpResponseRedirect()也是硬编码转向地址，当然在template 中也是一样了，这样带来一个问题，如果在urls.py 中修改了某个页面的地址（也就是说更改路由系统中对应的路由分发），那么所有的地方(views.py和template中)都要修改。问题出在硬编码，紧耦合使得在大量的模板中修改 URLs 成为富有挑战性的项目。来看下面的模板文件index.html中，我们到的链接硬编码成这样子：
&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;/goods/index/&amp;quot;&amp;gt;url硬编码&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;  如果使用软编码之后，无论怎么更改路由系统中的路由分发，只有对应的namespace与name属性值不变，就不必修改在views.</description>
    </item>
    
    <item>
      <title>redis和memeuche</title>
      <link>/2018/redis%E5%92%8Cmemeuche/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/redis%E5%92%8Cmemeuche/</guid>
      <description>redis是什么？ Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。
是当前最热门的的的NoSql数据库之一，也被人们称为数据结构服务器。
为什么要用redis? 原因很简单：快、性能好、支持并发
这个问题在大并发，高负载的网站中必须考虑.redis数据库中的所有数据都存储在内存中。由于内存的读写速度远快于硬盘，因此Redis的的的在性能上对比其他基于硬盘存储的数据库有非常明显的优势。
性能 我们在碰到需要执行耗时特别久，且结果不频繁变动的SQL，就特别适合将运行结果放入缓存，这样，后面的请求就去缓存中读取，请求使得能够迅速响应。
并发 在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用的的Redis的做一个缓冲操作，让请求先访问到的Redis的的，而不是直接访问数据库。
redis的优点： 1，运行在内存，速度快官方号称支持并发11万特读操作，并发8万特写操作。
2，数据虽在内存，但是提供了持久化的支持，即可以将内存中的数据异步写入到硬盘中，同时不影响继续提供服务。
3，支持数据结构丰富（string（字符串），list（链表），set（集合），zset（sorted set - 有序集合））和Hash（哈希类型，md5加密出来的那个串）
——————————————————————————————————————
Memcached介绍 Memcached 是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提供动态、数据库驱动网站的速度，现在已被LiveJournal、hatena、Facebook、Vox、LiveJournal等公司所使用。
Memcached工作方式分析 许多Web应用都将数据保存到 RDBMS中，应用服务器从中读取数据并在浏览器中显示。 但随着数据量的增大、访问的集中，就会出现RDBMS的负担加重、数据库响应恶化、 网站显示延迟等重大影响。Memcached是高性能的分布式内存缓存服务器,通过缓存数据库查询结果，减少数据库访问次数，以提高动态Web等应用的速度、 提高可扩展性。下图展示了memcache与数据库端协同工作情况： 其中的过程是这样的： 1.检查用户请求的数据是缓存中是否有存在，如果有存在的话，只需要直接把请求的数据返回，无需查询数据库。 2.如果请求的数据在缓存中找不到，这时候再去查询数据库。返回请求数据的同时，把数据存储到缓存中一份。 3.保持缓存的“新鲜性”，每当数据发生变化的时候（比如，数据有被修改，或被删除的情况下），要同步的更新缓存信息，确保用户不会在缓存取到旧的数据。
Memcached作为高速运行的分布式缓存服务器，具有以下的特点： 1.协议简单 2.基于libevent的事件处理 3.内置内存存储方式 4.memcached不互相通信的分布式
如何实现分布式可拓展性？ Memcached的分布式不是在服务器端实现的，而是在客户端应用中实现的，即通过内置算法制定目标数据的节点，如下图所示： ——————————————————————————————————————
Redis 介绍 Redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、 list(链表)、set(集合)和zset(有序集合)。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步,当前 Redis的应用已经非常广泛，国内像新浪、淘宝，国外像 Flickr、Github等均在使用Redis的缓存服务。
Redis 工作方式分析 Redis作为一个高性能的key-value数据库具有以下特征： 1.多样的数据模型 2.持久化 3.主从同步 Redis支持丰富的数据类型，最为常用的数据类型主要由五种：String、Hash、List、Set和Sorted Set。Redis通常将数据存储于内存中，或被配置为使用虚拟内存。Redis有一个很重要的特点就是它可以实现持久化数据，通过两种方式可以实现数据持久化：使用RDB快照的方式，将内存中的数据不断写入磁盘；或使用类似MySQL的AOF日志方式，记录每次更新的日志。前者性能较高，但是可能会引起一定程度的数据丢失；后者相反。 Redis支持将数据同步到多台从数据库上，这种特性对提高读取性能非常有益。
Redis如何实现分布式可拓展性？ 2.8以前的版本：与Memcached一致，可以在客户端实现，也可以使用代理，twitter已开发出用于Redis和Memcached的代理Twemproxy 。 3.0 以后的版本：相较于Memcached只能采用客户端实现分布式存储，Redis则在服务器端构建分布式存储。Redis Cluster是一个实现了分布式且允许单点故障的Redis高级版本，它没有中心节点，各个节点地位一致，具有线性可伸缩的功能。如图给出Redis Cluster的分布式存储架构，其中节点与节点之间通过二进制协议进行通信，节点与客户端之间通过ascii协议进行通信。在数据的放置策略上，Redis Cluster将整个 key的数值域分成16384个哈希槽，每个节点上可以存储一个或多个哈希槽，也就是说当前Redis Cluster支持的最大节点数就是16384。 ——————————————————————————————————————
1、 Redis和Memcache都是将数据存放在内存中，都是内存数据库。不过memcache还可用于缓存其他东西，例如图片、视频等等。 2、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash等数据结构的存储。 3、虚拟内存–Redis当物理内存用完时，可以将一些很久没用到的value 交换到磁盘 4、过期策略–memcache在set时就指定，例如set key1 0 0 8,即永不过期。Redis可以通过例如expire 设定，例如expire name 10 5、分布式–设定memcache集群，利用magent做一主多从;redis可以做一主多从。都可以一主一从 6、存储数据安全–memcache挂掉后，数据没了；redis可以定期保存到磁盘（持久化） 7、灾难恢复–memcache挂掉后，数据不可恢复; redis数据丢失后可以通过aof恢复 8、Redis支持数据的备份，即master-slave模式的数据备份。</description>
    </item>
    
    <item>
      <title>简单认识下Kafka</title>
      <link>/2018/%E7%AE%80%E5%8D%95%E8%AE%A4%E8%AF%86%E4%B8%8Bkafka/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/%E7%AE%80%E5%8D%95%E8%AE%A4%E8%AF%86%E4%B8%8Bkafka/</guid>
      <description>Kafka的消息是保存或缓存在磁盘上的，一般认为在磁盘上读写数据是会降低性能的，因为寻址会比较消耗时间，但是实际上，Kafka的特性之一就是高吞吐率。
即使是普通的服务器，Kafka也可以轻松支持每秒百万级的写入请求，超过了大部分的消息中间件，这种特性也使得Kafka在日志处理等海量数据场景广泛应用。
针对Kafka的基准测试可以参考，Apache Kafka基准测试：每秒写入2百万（在三台廉价机器上）
下面从数据写入和读取两方面分析，为什么Kafka速度这么快。
一、写入数据
Kafka会把收到的消息都写入到硬盘中，它绝对不会丢失数据。为了优化写入速度Kafka采用了两个技术， 顺序写入和MMFile 。
1、顺序写入
磁盘读写的快慢取决于你怎么使用它，也就是顺序读写或者随机读写。在顺序读写的情况下，磁盘的顺序读写速度和内存持平。
因为硬盘是机械结构，每次读写都会寻址-&amp;gt;写入，其中寻址是一个“机械动作”，它是最耗时的。所以硬盘最讨厌随机I/O，最喜欢顺序I/O。为了提高读写硬盘的速度，Kafka就是使用顺序I/O。
而且Linux对于磁盘的读写优化也比较多，包括read-ahead和write-behind，磁盘缓存等。如果在内存做这些操作的时候，一个是JAVA对象的内存开销很大，另一个是随着堆内存数据的增多，JAVA的GC时间会变得很长，使用磁盘操作有以下几个好处：
 磁盘顺序读写速度超过内存随机读写 JVM的GC效率低，内存占用大。使用磁盘可以避免这一问题 系统冷启动后，磁盘缓存依然可用  下图就示了Kafka是如何写入数据的， 每一个Partition其实都是一个文件 ，收到消息后Kafka会把数据插入到文件末尾（虚框部分）：
这种方法有一个缺陷——没有办法删除数据 ，所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个Topic都有一个offset用来表示读取到了第几条数据 。
两个消费者：
 Consumer1有两个offset分别对应Partition0、Partition1（假设每一个Topic一个Partition）； Consumer2有一个offset对应Partition2。  这个offset是由客户端SDK负责保存的，Kafka的Broker完全无视这个东西的存在；一般情况下SDK会把它保存到Zookeeper里面，所以需要给Consumer提供zookeeper的地址。
如果不删除硬盘肯定会被撑满，所以Kakfa提供了两种策略来删除数据：
 一是基于时间； 二是基于partition文件大小。  具体配置可以参看它的配置文档。
2、Memory Mapped Files
即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以Kafka的数据并不是实时的写入硬盘 ，它充分利用了现代操作系统分页存储来利用内存提高I/O效率。
Memory Mapped Files(后面简称mmap)也被翻译成 内存映射文件 ，在64位操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page来实现文件到物理内存的直接映射。
完成映射之后你对物理内存的操作会被同步到硬盘上（操作系统在适当的时候）。
通过mmap，进程像读写硬盘一样读写内存（当然是虚拟机内存），也不必关心内存的大小有虚拟内存为我们兜底。
使用这种方式可以获取很大的I/O提升，省去了用户空间到内核空间复制的开销（调用文件的read会把数据先放到内核空间的内存中，然后再复制到用户空间的内存中。）
但也有一个很明显的缺陷——不可靠，写到mmap中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用flush的时候才把数据真正的写到硬盘。
Kafka提供了一个参数——producer.type来控制是不是主动flush，如果Kafka写入到mmap之后就立即flush然后再返回Producer叫 同步 (sync)；写入mmap之后立即返回Producer不调用flush叫异步 (async)。
二、读取数据
Kafka在读取磁盘时做了哪些优化？
2、基于sendfile实现Zero Copy
传统模式下，当需要对一个文件进行传输的时候，其具体流程细节如下：
 调用read函数，文件数据被copy到内核缓冲区 read函数返回，文件数据从内核缓冲区copy到用户缓冲区 write函数调用，将文件数据从用户缓冲区copy到内核与socket相关的缓冲区。 数据从socket缓冲区copy到相关协议引擎。  以上细节是传统read/write方式进行网络文件传输的方式，我们可以看到，在这个过程当中，文件数据实际上是经过了四次copy操作：
硬盘—&amp;gt;内核buf—&amp;gt;用户buf—&amp;gt;socket相关缓冲区—&amp;gt;协议引擎
而sendfile系统调用则提供了一种减少以上多次copy，提升文件传输性能的方法。
在内核版本2.1中，引入了sendfile系统调用，以简化网络上和两个本地文件之间的数据传输。sendfile的引入不仅减少了数据复制，还减少了上下文切换。
sendfile(socket, file, len);</description>
    </item>
    
    <item>
      <title>装饰器</title>
      <link>/2018/%E8%A3%85%E9%A5%B0%E5%99%A8/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/%E8%A3%85%E9%A5%B0%E5%99%A8/</guid>
      <description>Kafka的消息是保存或缓存在磁盘上的，一般认为在磁盘上读写数据是会降低性能的，因为寻址会比较消耗时间，但是实际上，Kafka的特性之一就是高吞吐率。
即使是普通的服务器，Kafka也可以轻松支持每秒百万级的写入请求，超过了大部分的消息中间件，这种特性也使得Kafka在日志处理等海量数据场景广泛应用。
针对Kafka的基准测试可以参考，Apache Kafka基准测试：每秒写入2百万（在三台廉价机器上）
下面从数据写入和读取两方面分析，为什么Kafka速度这么快。
一、写入数据
Kafka会把收到的消息都写入到硬盘中，它绝对不会丢失数据。为了优化写入速度Kafka采用了两个技术， 顺序写入和MMFile 。
1、顺序写入
磁盘读写的快慢取决于你怎么使用它，也就是顺序读写或者随机读写。在顺序读写的情况下，磁盘的顺序读写速度和内存持平。
因为硬盘是机械结构，每次读写都会寻址-&amp;gt;写入，其中寻址是一个“机械动作”，它是最耗时的。所以硬盘最讨厌随机I/O，最喜欢顺序I/O。为了提高读写硬盘的速度，Kafka就是使用顺序I/O。
而且Linux对于磁盘的读写优化也比较多，包括read-ahead和write-behind，磁盘缓存等。如果在内存做这些操作的时候，一个是JAVA对象的内存开销很大，另一个是随着堆内存数据的增多，JAVA的GC时间会变得很长，使用磁盘操作有以下几个好处：
 磁盘顺序读写速度超过内存随机读写 JVM的GC效率低，内存占用大。使用磁盘可以避免这一问题 系统冷启动后，磁盘缓存依然可用  下图就示了Kafka是如何写入数据的， 每一个Partition其实都是一个文件 ，收到消息后Kafka会把数据插入到文件末尾（虚框部分）：
这种方法有一个缺陷——没有办法删除数据 ，所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个Topic都有一个offset用来表示读取到了第几条数据 。
两个消费者：
 Consumer1有两个offset分别对应Partition0、Partition1（假设每一个Topic一个Partition）； Consumer2有一个offset对应Partition2。  这个offset是由客户端SDK负责保存的，Kafka的Broker完全无视这个东西的存在；一般情况下SDK会把它保存到Zookeeper里面，所以需要给Consumer提供zookeeper的地址。
如果不删除硬盘肯定会被撑满，所以Kakfa提供了两种策略来删除数据：
 一是基于时间； 二是基于partition文件大小。  具体配置可以参看它的配置文档。
2、Memory Mapped Files
即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以Kafka的数据并不是实时的写入硬盘 ，它充分利用了现代操作系统分页存储来利用内存提高I/O效率。
Memory Mapped Files(后面简称mmap)也被翻译成 内存映射文件 ，在64位操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page来实现文件到物理内存的直接映射。
完成映射之后你对物理内存的操作会被同步到硬盘上（操作系统在适当的时候）。
通过mmap，进程像读写硬盘一样读写内存（当然是虚拟机内存），也不必关心内存的大小有虚拟内存为我们兜底。
使用这种方式可以获取很大的I/O提升，省去了用户空间到内核空间复制的开销（调用文件的read会把数据先放到内核空间的内存中，然后再复制到用户空间的内存中。）
但也有一个很明显的缺陷——不可靠，写到mmap中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用flush的时候才把数据真正的写到硬盘。
Kafka提供了一个参数——producer.type来控制是不是主动flush，如果Kafka写入到mmap之后就立即flush然后再返回Producer叫 同步 (sync)；写入mmap之后立即返回Producer不调用flush叫异步 (async)。
二、读取数据
Kafka在读取磁盘时做了哪些优化？
2、基于sendfile实现Zero Copy
传统模式下，当需要对一个文件进行传输的时候，其具体流程细节如下：
 调用read函数，文件数据被copy到内核缓冲区 read函数返回，文件数据从内核缓冲区copy到用户缓冲区 write函数调用，将文件数据从用户缓冲区copy到内核与socket相关的缓冲区。 数据从socket缓冲区copy到相关协议引擎。  以上细节是传统read/write方式进行网络文件传输的方式，我们可以看到，在这个过程当中，文件数据实际上是经过了四次copy操作：
硬盘—&amp;gt;内核buf—&amp;gt;用户buf—&amp;gt;socket相关缓冲区—&amp;gt;协议引擎
而sendfile系统调用则提供了一种减少以上多次copy，提升文件传输性能的方法。
在内核版本2.1中，引入了sendfile系统调用，以简化网络上和两个本地文件之间的数据传输。sendfile的引入不仅减少了数据复制，还减少了上下文切换。
sendfile(socket, file, len);</description>
    </item>
    
    <item>
      <title>分页优化</title>
      <link>/2018/%E5%88%86%E9%A1%B5%E4%BC%98%E5%8C%96/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/%E5%88%86%E9%A1%B5%E4%BC%98%E5%8C%96/</guid>
      <description>使用合理的分页方式以提高分页的效率
select id,name from product limit 866613, 20
使用上述SQL语句做分页的时候，可能有人会发现，随着表数据量的增加，直接使用limit分页查询会越来越慢。
优化的方法如下：可以取前一页的最大行数的id，然后根据这个最大的id来限制下一页的起点。比如此列中，上一页最大的id是866612。SQL可以采用如下的写法：
select id,name from product where id&amp;gt; 866612 limit 20
△分段查询 在一些用户选择页面中，可能一些用户选择的时间范围过大，造成查询缓慢。主要的原因是扫描行数过多。这个时候可以通过程序，分段进行查询，循环遍历，将结果合并处理进行展示。
扫描的行数成百万级以上的时候就可以使用分段查询</description>
    </item>
    
    <item>
      <title>Vue 组件</title>
      <link>/2018/vue%E7%BB%84%E4%BB%B6/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/vue%E7%BB%84%E4%BB%B6/</guid>
      <description>基本示例
这里有一个 Vue 组件的示例：
// 定义一个名为 button-counter 的新组件 Vue.component(&amp;lsquo;button-counter&amp;rsquo;, { data: function () { return { count: 0 } }, template: &amp;lsquo;You clicked me {{ count }} times.&amp;rsquo; })
组件是可复用的 Vue 实例，且带有一个名字：在这个例子中是 。我们可以在一个通过 new Vue 创建的 Vue 根实例中，把这个组件作为自定义元素来使用：
  new Vue({ el: &amp;lsquo;#components-demo&amp;rsquo; })
You clicked me 0 times.
因为组件是可复用的 Vue 实例，所以它们与 new Vue 接收相同的选项，例如 data、computed、watch、methods 以及生命周期钩子等。仅有的例外是像 el这样根实例特有的选项。
组件的复用
你可以将组件进行任意次数的复用：
    You clicked me 0 times. You clicked me 0 times.</description>
    </item>
    
    <item>
      <title>Vue 过滤器</title>
      <link>/2018/vue%E8%BF%87%E6%BB%A4%E5%99%A8/</link>
      <pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/vue%E8%BF%87%E6%BB%A4%E5%99%A8/</guid>
      <description>过滤器：生活中有很多例子，净水器 空气净化器 。 过滤器的作用：实现数据的筛选、过滤、格式化。
vue1.*版本是有内置的过滤器，但是在vue2.*所有的版本都已经没有自带的过滤器了。
1、过滤器创建 过滤器的本质 是一个有参数 有返回值的方法 Vue.filter(&amp;lsquo;cap&amp;rsquo;,function(value){
​ 过滤器逻辑
​ });
2、过滤器使用 语法： {{表达式 | 过滤器}} 举个例子： {{price | cap}}
3、过滤器高级用法
在使用过滤器的时候，还可以指定参数，来告诉过滤器按照参数进行数据的过滤。
①如何给过滤器传参？ {{price | cap (&amp;lsquo;￥&amp;rsquo;,true)}} ②如何在过滤器中接收到？ new Vue({ filters:{ //myInput是通过管道传来的数据 //arg1在调用过滤器时在圆括号中第一个参数 //arg2在调用过滤器时在圆括号中第二个参数 cap:function(myInput,arg1,arg2){ return 处理后的数据 }
　}
})</description>
    </item>
    
    <item>
      <title>Class 与 Style 绑定</title>
      <link>/2018/vue-class%E4%B8%8Estyle%E7%BB%91%E5%AE%9A/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/vue-class%E4%B8%8Estyle%E7%BB%91%E5%AE%9A/</guid>
      <description>操作元素的 class 列表和内联样式是数据绑定的一个常见需求。因为它们都是属性，所以我们可以用 v-bind 处理它们：只需要通过表达式计算出字符串结果即可。不过，字符串拼接麻烦且易错。因此，在将 v-bind 用于 class 和 style 时，Vue.js 做了专门的增强。表达式结果的类型除了字符串之外，还可以是对象或数组。
我们可以传给 v-bind:class 一个对象，以动态地切换 class：
 上面的语法表示 active 这个 class 存在与否将取决于数据属性 isActive 。
你可以在对象中传入更多属性来动态切换多个 class。此外，v-bind:class 指令也可以与普通的 class 属性共存。当有如下模板:
 和如下 data：
data: { isActive: true, hasError: false }
结果渲染为：
 当 isActive 或者 hasError 变化时，class 列表将相应地更新。例如，如果 hasError 的值为 true，class 列表将变为 &amp;ldquo;static active text-danger&amp;rdquo;。
绑定的数据对象不必内联定义在模板里：
no data: { classObject: { active: true, &#39;text-danger&#39;: false } } 渲染的结果和上面一样。我们也可以在这里绑定一个返回对象的**计算属****性**。这是一个常用且强大的模式：  data: { isActive: true, error: null }, computed: { classObject: function () { return { active: this.</description>
    </item>
    
    <item>
      <title>docker安装</title>
      <link>/2018/docker%E5%AE%89%E8%A3%85./</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/docker%E5%AE%89%E8%A3%85./</guid>
      <description>==1安装docker== 安装一些必要的系统工具：
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
添加软件源信息：
sudo yum-config-manager &amp;ndash;add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
更新 yum 缓存：
sudo yum makecache fast
安装 Docker-ce：
sudo yum -y install docker-ce
启动 Docker 后台服务
sudo systemctl start docker
测试运行 hello-world
docker run hello-world
查看安装版本
docker version
2删除 Docker CE 执行以下命令来删除 Docker CE：
$ sudo yum remove docker-ce $ sudo rm -rf /var/lib/docker  ==为了避免每次命令都输入sudo，可以设置用户权限，注意执行后须注销重新登录==
sudo usermod -a -G docker $USER
3.Docker启动与停止 安装完成Docker后，默认已经启动了docker服务，如需手动控制docker服务的启停，可执行如下命令</description>
    </item>
    
    <item>
      <title>docker的卸载</title>
      <link>/2018/docker%E7%9A%84%E5%8D%B8%E8%BD%BD/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/docker%E7%9A%84%E5%8D%B8%E8%BD%BD/</guid>
      <description>卸载
列出你安装过的包
[root@ ]# yum list installed | grep docker containerd.io.x86_64 1.2.5-3.1.el7 @docker-ce-stable docker-ce-cli.x86_64 1:18.09.6-3.el7 @docker-ce-stable  删除安装包
[root@ ]# sudo yum -y remove docker-ce-cli.x86_64  删除镜像/容器
yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine rm -rf /etc/systemd/system/docker.service.d rm -rf /var/lib/docker rm -rf /var/run/docker  </description>
    </item>
    
    <item>
      <title>Vue 生命周期</title>
      <link>/2018/vue%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</link>
      <pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/vue%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</guid>
      <description>Vue实例有一个完整的生命周期，也就是从开始创建、初始化数据、编译模板、挂载Dom、渲染→更新→渲染、销毁等一系列过程，我们称这是Vue的生命周期。通俗说就是Vue实例从创建到销毁的过程，就是生命周期。
每一个组件或者实例都会经历一个完整的生命周期，总共分为三个阶段：初始化、运行中、销毁。
 实例、组件通过new Vue() 创建出来之后会初始化事件和生命周期，然后就会执行beforeCreate钩子函数，这个时候，数据还没有挂载呢，只是一个空壳，无法访问到数据和真实的dom，一般不做操作
 挂载数据，绑定事件等等，然后执行created函数，这个时候已经可以使用到数据，也可以更改数据,在这里更改数据不会触发updated函数，在这里可以在渲染前倒数第二次更改数据的机会，不会触发其他的钩子函数，一般可以在这里做初始数据的获取
 接下来开始找实例或者组件对应的模板，编译模板为虚拟dom放入到render函数中准备渲染，然后执行beforeMount钩子函数，在这个函数中虚拟dom已经创建完成，马上就要渲染,在这里也可以更改数据，不会触发updated，在这里可以在渲染前最后一次更改数据的机会，不会触发其他的钩子函数，一般可以在这里做初始数据的获取
 接下来开始render，渲染出真实dom，然后执行mounted钩子函数，此时，组件已经出现在页面中，数据、真实dom都已经处理好了,事件都已经挂载好了，可以在这里操作真实dom等事情&amp;hellip;
 当组件或实例的数据更改之后，会立即执行beforeUpdate，然后vue的虚拟dom机制会重新构建虚拟dom与上一次的虚拟dom树利用diff算法进行对比之后重新渲染，一般不做什么事儿
 当更新完成后，执行updated，数据已经更改完成，dom也重新render完成，可以操作更新后的虚拟dom
 当经过某种途径调用$destroy方法后，立即执行beforeDestroy，一般在这里做一些善后工作，例如清除计时器、清除非指令绑定的事件等等
 组件的数据绑定、监听&amp;hellip;去掉后只剩下dom空壳，这个时候，执行destroyed，在这里做善后工作也可以
  </description>
    </item>
    
    <item>
      <title>django密码加密</title>
      <link>/2017/django%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86/</link>
      <pubDate>Sat, 09 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/django%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86/</guid>
      <description>python的Django框架自带了一套加密方法: make_password( ),具体作用如下:
&amp;gt;&amp;gt;&amp;gt; from django.contrib.auth.hashers import make_password, check_password &amp;gt;&amp;gt;&amp;gt; make_password(&amp;quot;www.baidu.com&amp;quot;, None, &#39;pbkdf2_sha1&#39;)  参1: 加密的密码,参2: 任意字符串(用于固定生成的字符串,不能为空) 参3: 加密方式
得到的是一串随机字符串,并且每次生成都不一样
&amp;gt;&amp;gt;&amp;gt; make_password(&amp;quot;abcdef&amp;quot;, None, &#39;pbkdf2_sha256&#39;) == make_password(&amp;quot;abcdef&amp;quot;, None,&#39;pbkdf2_sha256&#39;) False  这时候如果需要验证密码比较是否正确就需要用check_password( ),具体如下:
&amp;gt;&amp;gt;&amp;gt; text = &amp;quot;www.baidu.com&amp;quot; &amp;gt;&amp;gt;&amp;gt; passwd = make_password(text, None, &#39;pbkdf2_sha1&#39;) &amp;gt;&amp;gt;&amp;gt; print passwd pbkdf2_sha112000xzMLhCNvQbb8$i1XDnJIpb/cRRGRX2x7Ym74RNfPRCUp5pbU6Sn+V3J0= &amp;gt;&amp;gt;&amp;gt; ret = check_password(text, passwd) &amp;gt;&amp;gt;&amp;gt; print(ret) True  如果你不想每次都生成不同的密文，可以把make_password的第二个函数给一个固定的字符串,如下:
&amp;gt;&amp;gt;&amp;gt; make_password(text,&amp;quot;a&amp;quot;,&#39;pbkdf2_sha1&#39;) u&#39;pbkdf2_sha112000a5HkIPczRZGSTKUBa5uzZmRuAWdp2Qe6Oemhdasvzv4Q=&#39; &amp;gt;&amp;gt;&amp;gt; make_password(text,&amp;quot;a&amp;quot;,&#39;pbkdf2_sha1&#39;) u′pbkdf2_sha112000a5HkIPczRZGSTKUBa5uzZmRuAWdp2Qe6Oemhdasvzv4Q=&#39;  </description>
    </item>
    
    <item>
      <title>docker操作</title>
      <link>/2017/%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Wed, 06 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C/</guid>
      <description>重新启动storage容器
[root@localhost ~]# docker stop storage [root@localhost ~]# docker start storage
查看tracker容器和storage容器的关联
[root@localhost ~]# docker exec -it storage bash root@localhost:/# cd fdfs_conf root@localhost:/fdfs_conf# fdfs_monitor storage.conf
查看docker运行的盒子
docker container ls
dicker images
重启nginx,开始
systemctl status nginx.service
systemctl status nginx.service</description>
    </item>
    
    <item>
      <title>Django增删改查（CUSD）</title>
      <link>/2017/djangomysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%A5%E5%BF%97/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/djangomysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%A5%E5%BF%97/</guid>
      <description>查看mysql数据库日志可以查看对数据库的操作记录。 mysql日志文件默认没有产生，需要做如下配置：
sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf  把68，69行前面的#去除，然后保存并使用如下命令重启mysql服务。
sudo service mysql restart  使用如下命令打开mysql日志文件。
tail -f /var/log/mysql/mysql.log # 可以实时查看数据库的日志内容 # 如提示需要sudo权限，执行 # sudo tail -f /var/log/mysql/mysql.log  </description>
    </item>
    
    <item>
      <title>Django的定义模型类</title>
      <link>/2017/django%E7%9A%84%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E7%B1%BB/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/django%E7%9A%84%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E7%B1%BB/</guid>
      <description>模型类被定义在&amp;rdquo;应用/models.py&amp;rdquo;文件中。 模型类必须继承自Model类，位于包django.db.models中。  接下来首先以&amp;rdquo;图书-人物&amp;rdquo;管理为例进行演示。
1 定义 在models.py 文件中定义模型类。
from django.db import models # Create your models here. # 准备书籍列表信息的模型类 class BookInfo(models.Model): # 创建字段，字段类型... name = models.CharField(max_length=20, verbose_name=&#39;名称&#39;) pub_date = models.DateField(verbose_name=&#39;发布日期&#39;,null=True) readcount = models.IntegerField(default=0, verbose_name=&#39;阅读量&#39;) commentcount = models.IntegerField(default=0, verbose_name=&#39;评论量&#39;) is_delete = models.BooleanField(default=False, verbose_name=&#39;逻辑删除&#39;) class Meta: db_table = &#39;bookinfo&#39; # 指明数据库表名 verbose_name = &#39;图书&#39; # 在admin站点中显示的名称 def __str__(self): &amp;quot;&amp;quot;&amp;quot;定义每个数据对象的显示信息&amp;quot;&amp;quot;&amp;quot; return self.name # 准备人物列表信息的模型类 class PeopleInfo(models.Model): GENDER_CHOICES = ( (0, &#39;male&#39;), (1, &#39;female&#39;) ) name = models.</description>
    </item>
    
    <item>
      <title>Django项目是如何开始的</title>
      <link>/2017/django%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%A7%8B/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/django%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%A7%8B/</guid>
      <description>1.创建项目
 django-admin startproject bookmanager   2.创建应用  python manager.py startapp book  3.更换python解释器：按需选择
  # 进入指定虚拟环境 which python # python2 /home/python/.virtualenvs/py_django/bin/python # python3 /home/python/.virtualenvs/py3_django/bin/python   4.安装应用  INSTALLED_APPS = [ &#39;django.contrib.admin&#39;, &#39;django.contrib.auth&#39;, &#39;django.contrib.contenttypes&#39;, &#39;django.contrib.sessions&#39;, &#39;django.contrib.messages&#39;, &#39;django.contrib.staticfiles&#39;, #添加子应用 &#39;book.apps.BookConfig&#39; ]   5.本地化  #设置中文 LANGUAGE_CODE = &#39;zh-Hans&#39; #亚洲上海时区 TIME_ZONE = &#39;Asia/Shanghai&#39;   6.模板路径
 在应用同级目录下,创建templates模板文件夹  TEMPLATES = [ { &#39;BACKEND&#39;: &#39;django.template.backends.django.DjangoTemplates&#39;, &#39;DIRS&#39;: [os.path.join(BASE_DIR,&#39;templates&#39;)], &#39;APP_DIRS&#39;: True, &#39;OPTIONS&#39;: { &#39;context_processors&#39;: [ &#39;django.</description>
    </item>
    
    <item>
      <title>Mysql索引</title>
      <link>/2017/mysql%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/mysql%E7%B4%A2%E5%BC%95/</guid>
      <description>索引是什么
索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。
更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度
索引目的
索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？
索引原理
除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。
数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(&amp;gt;、&amp;lt;、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。
普通索引
创建索引
这是最基本的索引，它没有任何限制。它有以下几种创建方式：
CREATE INDEX indexName ON mytable(username(length));
如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。
修改表结构(添加索引)
ALTER table tableName ADD INDEX indexName(columnName)
创建表的时候直接指定
CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX indexName );
删除索引的语法
DROP INDEX [indexName] ON mytable;
查看索引
show index from 表名;
用来查看某个表都具备哪些索引
联合索引
为两个或者两个以上的字段创建普通索引，适用于多个字段联合查询的优化
创建联合索引
ALTER TABLE test_index
ADD INDEX s_index (id, name) ;</description>
    </item>
    
    <item>
      <title>DjangoApp应用配置</title>
      <link>/2017/djangoapp%E5%BA%94%E7%94%A8/</link>
      <pubDate>Sat, 21 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/djangoapp%E5%BA%94%E7%94%A8/</guid>
      <description>在每个应用目录中都包含了apps.py文件，用于保存该应用的相关信息。
在创建应用时，Django会向apps.py文件中写入一个该应用的配置类，如
from django.apps import AppConfig class BookConfig(AppConfig): name = &#39;book&#39;  我们将此类添加到工程settings.py中的INSTALLED_APPS列表中，表明注册安装具备此配置属性的应用。
 AppConfig.name属性表示这个配置类是加载到哪个应用的，每个配置类必须包含此属性，默认自动生成。
 AppConfig.verbose_name属性用于设置该应用的直观可读的名字，此名字在Django提供的Admin管理站点中会显示，如
  from django.apps import AppConfig class UsersConfig(AppConfig): name = &#39;book&#39; verbose_name = &#39;图书管理&#39;  </description>
    </item>
    
    <item>
      <title>Mysql事务细说</title>
      <link>/2017/mysql%E4%BA%8B%E5%8A%A1/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/mysql%E4%BA%8B%E5%8A%A1/</guid>
      <description>原子性（atomicity）
一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作，这就是事务的原子性
一致性（consistency）
数据库总是从一个一致性的状态转换到另一个一致性的状态。（在前面的例子中，一致性确保了，即使在执行第三、四条语句之间时系统崩溃，支票账户中也不会损失200美元，因为事务最终没有提交，所以事务中所做的修改也不会保存到数据库中。）
隔离性（isolation）
通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。（在前面的例子中，当执行完第三条语句、第四条语句还未开始时，此时有另外的一个账户汇总程序开始运行，则其看到支票帐户的余额并没有被减去200美元。）
持久性（durability）
一旦事务提交，则其所做的修改会永久保存到数据库。（此时即使系统崩溃，修改的数据也不会丢失。）
事务命令
表的引擎类型必须是innodb类型才可以使用事务，这是mysql表的默认引擎
查看表的创建语句，可以看到engine=innodb
&amp;ndash; 选择数据库
use jing_dong;
&amp;ndash; 查看goods表
show create table goods;
开启事务，命令如下：
开启事务后执行修改命令，变更会维护到本地缓存中，而不维护到物理表中
begin;
或者
start transaction;
提交事务，命令如下
commit;
将缓存中的数据变更维护到物理表中
回滚事务，命令如下：
放弃缓存中变更的数据
rollback;</description>
    </item>
    
    <item>
      <title>Mysql 自关联</title>
      <link>/2017/mysql%E8%87%AA%E5%85%B3%E8%81%94/</link>
      <pubDate>Wed, 18 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/mysql%E8%87%AA%E5%85%B3%E8%81%94/</guid>
      <description>自关联的基本含义就是自己join自己，用一张表搞定多张表可以达到的效果
需要注意的是，当表自关联的时候，需要使用as 起别名，防止关联的时候引发重名的问题。
自关联
select
​ *
from
​ areas as p
inner join areas as c on c.pid=p.aid
where
​ p.atitle=&amp;lsquo;河南省&amp;rsquo;;
查询所有区
select
​ *
from
​ areas as c
inner join areas as a on a.pid=c.aid
where
​ c.atitle=&amp;lsquo;郑州市&amp;rsquo;;
三表关联
select
​ *
from
​ areas as p
left join areas as c on c.pid=p.aid
left join areas as a on a.pid=c.aid
where
p.atitle=&amp;lsquo;河南省&amp;rsquo;
子查询的定义
子查询允许把一个查询嵌套在另一个查询当中。
 标量子查询：  　是指子查询返回的是单一值的标量，如一个数字或一个字符串，也是子查询中最简单的返回形式。 可以使用 = &amp;gt; &amp;lt; &amp;gt;= &amp;lt;= &amp;lt;&amp;gt; 这些操作符对子查询的标量结果进行比较，通常子查询的位置在比较式的右侧</description>
    </item>
    
    <item>
      <title>Django视图和url</title>
      <link>/2017/django%E7%9A%84%E8%A7%86%E5%9B%BE%E5%92%8Curl/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/django%E7%9A%84%E8%A7%86%E5%9B%BE%E5%92%8Curl/</guid>
      <description>1. 定义视图  视图就是一个Python函数，被定义在应用的views.py中.
 视图的第一个参数是HttpRequest类型的对象reqeust，包含了所有请求信息.
 视图必须返回HttpResponse对象，包含返回给请求者的响应信息.
 需要导入HttpResponse模块 :from django.http import HttpResponse
 定义视图函数 : 响应字符串OK!给客户端
   思考 : 如何才能让请求找到视图?
 2. 配置URLconf  查找视图的过程 :
 1.请求者在浏览器地址栏中输入URL, 请求到网站.
 2.网站获取URL信息.
 3.然后与编写好的URLconf逐条匹配.
 4.如果匹配成功则调用对应的视图.
 5.如果所有的URLconf都没有匹配成功.则返回404错误.
   URLconf入口
   需要两步完成URLconf配置
 1.在项目中定义URLconf 2.在应用中定义URLconf  在项目中定义URLconf
   在应用中定义URLconf
 提示：一条URLconf包括URL规则、视图两部分
 URL规则使用正则表达式定义.
 视图就是在views.py中定义的视图函数.
   url匹配过程
    3.</description>
    </item>
    
    <item>
      <title>Mysql建表建库</title>
      <link>/2017/mysql%E5%AE%89%E8%A3%85%E5%BB%BA%E8%A1%A8%E5%BB%BA%E5%BA%93/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/mysql%E5%AE%89%E8%A3%85%E5%BB%BA%E8%A1%A8%E5%BB%BA%E5%BA%93/</guid>
      <description>1 安装navicat 和 xampp ，注意路径不要出现中文，xampp最好安装在根目录
比如 d:/xampp
mysql 默认运行在3306端口
命令查看是否运行：netstat -an|find &amp;ldquo;3306&amp;rdquo; mysql的配置文件是my.ini 可以用来修改mysql的端口，目录，以及相关的配置
启动mysql 使用start 停止使用 stop
进入mysql命令行，找到你的xampp目录地址，进入mysql/bin目录打命令
mysql –uroot –p123456
修改root的密码
use mysql;update user set password = password(‘123456’) where user = ‘root’
退出mysql命令用 quit 或者 exit
mysql 默认的数据引擎是 innodb
InnoDB：支持事务处理，支持外键，并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）。
MyISAM：插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择MyISAM能实现处理高效率。如果应用的完整性、并发性要求比较低，也可以使用。
查看mysql 当前版本： select verison(); 查看当前时间： select now();
查看逻辑运算：select 1+1;
列出所有数据库：show databases;
创建数据库 （一定注意创建数据库要指定编码utf8）
create database 库名 charset=utf8;
删除数据库
drop database 库名;
选择数据库
use 库名;
列出当前数据库中的所有表
show tables;</description>
    </item>
    
    <item>
      <title>Django站点管理</title>
      <link>/2017/django%E7%AB%99%E7%82%B9%E7%AE%A1%E7%90%86/</link>
      <pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/django%E7%AB%99%E7%82%B9%E7%AE%A1%E7%90%86/</guid>
      <description>站点: 分为内容发布和公共访问两部分
 内容发布的部分由网站的管理员负责查看、添加、修改、删除数据
 Django能够根据定义的模型类自动地生成管理模块
 使用
  Django  的管理模块, 需要按照如下步骤操作 :
 1.管理界面本地化 2.创建管理员 3.注册模型类 4.发布内容到数据库  1.管理界面本地化  本地化是将显示的语言、时间等使用本地的习惯，这里的本地化就是进行中国化.
 中国大陆地区使用简体中文, 时区使用亚洲/上海时区, 注意这里不使用北京时区.
  2.创建管理员  创建管理员的命令 :  python manage.py createsuperuser   按提示输入用户名、邮箱、密码
 重置密码
 python manager.py changepassword 用户名   登陆站点 :http://127.0.0.1:8000/admin
 需要服务器是启动状态
 登陆站点成功
 站点界面中没有书籍和人物管理入口,因为没有注册模型类
  3.注册模型类  在应用的admin.py文件中注册模型类
 需要导入模型模块 :from book.models import BookInfo,PeopleInfo</description>
    </item>
    
    <item>
      <title>连接查询(join 语法)</title>
      <link>/2017/mysql%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2/</link>
      <pubDate>Sun, 24 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/mysql%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2/</guid>
      <description>join 用于多表中字段之间的联系，语法如下：
&amp;hellip; FROM table1 INNER|LEFT|RIGHT JOIN table2 ON conditiona
table1:左表；table2:右表。
JOIN 按照功能大致分为如下三类：
INNER JOIN（内连接,或等值连接）：取得两个表中存在连接匹配关系的记录。
LEFT JOIN（左连接）：取得左表（table1）完全记录，即是右表（table2）并无对应匹配记录。
RIGHT JOIN（右连接）：与 LEFT JOIN 相反，取得右表（table2）完全记录，即是左表（table1）并无匹配对应记录。
注意：mysql不支持Full join,不过可以通过UNION 关键字来合并 LEFT JOIN 与 RIGHT JOIN来模拟FULL join.
接下来给出一个列子用于解释下面几种分类。如下两个表(A,B)
 mysql&amp;gt; select A.id,A.name,B.name from A,B where A.id=B.id;
 +&amp;mdash;-+&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
 | id | name | name |
 +&amp;mdash;-+&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
 | 1 | Pirate | Rutabaga |
 | 2 | Monkey | Pirate |
 | 3 | Ninja | Darth Vader |</description>
    </item>
    
    <item>
      <title>Nginx的一些报错</title>
      <link>/2017/nginx%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%A5%E9%94%99/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/nginx%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%A5%E9%94%99/</guid>
      <description>nginx不像apache那样能识别.haccess文件,
需要配置,加以下语句加到nginx配置下
if (!-e $request_filename) { rewrite ^(.*)$ /index.php?s=/$1 last; break; }
查看自：https://blog.csdn.net/resilient/article/details/85318009</description>
    </item>
    
    <item>
      <title>Django创建项目</title>
      <link>/2017/django%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE/</link>
      <pubDate>Mon, 18 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/django%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE/</guid>
      <description> 创建Django项目 ```python
 django-admin startproject name
- **创建子应用** ```python - python manager.py startapp name    </description>
    </item>
    
    <item>
      <title>Mysql 数据类型聚合函数</title>
      <link>/2017/mysql%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/mysql%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</guid>
      <description>MySQL支持多种类型，大致可以分为三类：数值、日期/时间和字符串(字符)类型
一个英文字符等于一个字节，一个中文三个字节
数据库类型的属性，重点是主键（primary key）和自增长(auto_increment)，理解主键的含义
表中的每一行都应该具有可以唯一标识自己的一列(或一组列)。而这个承担标识作用的列称为主键。
如果没有主键，数据的管理将会十分混乱。比如会存在多条一模一样的记录，删除和修改特定行十分困难。
自增长字段每个表只能有一个
使用 as 给字段起别名(给长的字段取别名)
select id as code from student;可以通过 as 给表起别名
在select后面列前使用distinct可以消除重复的行
s
mysql的联合主键：用2个字段(或者多个字段,后面具体都是用2个字段组合)来确定一条记录，说明，这2个字段都不是唯一的，2个字段可以分别重复，这么设置的好处，可以很直观的看到某个重复字段的记录条数。
筛选条件格式
select 字段1，字段2 from 表名 where 条件
比较运算符
逻辑运算符
and or not （is 用来判断null）
例5：查询编号大于3的女同学
select id,name,gender from student where id&amp;gt;3 and gender=&amp;lsquo;女&amp;rsquo;
例6：查询编号小于4或没被删除的学生
select id,name,is_delete from student where id&amp;lt;4 or is_delete=0
例7：查询年龄不为空的学生
select id,name,age from student where age is not null
范围查询
between &amp;hellip; and (相当于&amp;lt;=和&amp;gt;=)
查询年龄在20岁到30岁之间的学生
select name from student where age between 20 and 30</description>
    </item>
    
    <item>
      <title>Django简介</title>
      <link>/2017/django%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/django%E7%AE%80%E4%BB%8B/</guid>
      <description>1. 简介 Django，发音为[`dʒæŋɡəʊ]，是用python语言写的开源web开发框架，并遵循MVC设计。劳伦斯出版集团为了开发以新闻内容为主的网站，而开发出来了这个框架，于2005年7月在BSD许可证下发布。这个名称来源于比利时的爵士音乐家DjangoReinhardt，他是一个吉普赛人，主要以演奏吉它为主，还演奏过小提琴等。由于Django在近年来的迅速发展，应用越来越广泛，被著名IT开发杂志SDTimes评选为2013SDTimes100，位列&amp;rdquo;API、库和框架&amp;rdquo;分类第6位，被认为是该领域的佼佼者。
Django的主要目的是简便、快速的开发数据库驱动的网站。它强调代码复用，多个组件可以很方便的以&amp;rdquo;插件&amp;rdquo;形式服务于整个框架，Django有许多功能强大的第三方插件，你甚至可以很方便的开发出自己的工具包。这使得Django具有很强的可扩展性。它还强调快速开发和DRY(DoNotRepeatYourself)原则。
2. 特点 1） 重量级框架 对比Flask框架，Django原生提供了众多的功能组件，让开发更简便快速。
 提供项目工程管理的自动化脚本工具 数据库ORM支持（对象关系映射，英语：Object Relational Mapping） 模板 表单 Admin管理站点 文件管理 认证权限 session机制 缓存  2）MVT模式 有一种程序设计模式叫MVC，其核心思想是分工、解耦，让不同的代码块之间降低耦合，增强代码的可扩展性和可移植性，实现向后兼容。
 MVC的全拼为Model-View-Controller，最早由TrygveReenskaug在1978年提出，是施乐帕罗奥多研究中心(Xerox PARC)在20世纪80年代为程序语言Smalltalk发明的一种软件设计模式，是为了将传统的输入（input）、处理（processing）、输出（output）任务运用到图形化用户交互模型中而设计的。随着标准输入输出设备的出现，开发人员只需要将精力集中在业务逻辑的分析与实现上。后来被推荐为Oracle旗下Sun公司Java EE平台的设计模式，并且受到越来越多的使用ColdFusion和PHP的开发者的欢迎。现在虽然不再使用原来的分工方式，但是这种分工的思想被沿用下来，广泛应用于软件工程中，是一种典型并且应用广泛的软件架构模式。后来，MVC的思想被应用在了Ｗeb开发方面，被称为Ｗeb MVC框架。
 MVC模式说明  M全拼为Model，主要封装对数据库层的访问，对数据库中的数据进行增、删、改、查操作。 V全拼为View，用于封装结果，生成页面展示的html内容。 C全拼为Controller，用于接收请求，处理业务逻辑，与Model和View交互，返回结果。  Django的MVT  M全拼为Model，与MVC中的M功能相同，负责和数据库交互，进行数据处理。 V全拼为View，与MVC中的C功能相同，接收请求，进行业务处理，返回应答。 T全拼为Template，与MVC中的V功能相同，负责封装构造要返回的html。  注：差异就在于黑线黑箭头标识出来的部分</description>
    </item>
    
    <item>
      <title>Flask wtform 组件</title>
      <link>/2017/flask%E7%9A%84wtform%E7%BB%84%E4%BB%B6/</link>
      <pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E7%9A%84wtform%E7%BB%84%E4%BB%B6/</guid>
      <description>WTForms是一个支持多个web框架的form组件，用于简化form表单的定义和一些操作
安装
pip3 install wtforms pip install flask-wtf  字段类型 说　明 StringField 文本字段 TextAreaField 多行文本字段 PasswordField 密码文本字段 HiddenField 隐藏文本字段 DateField 文本字段，值为 datetime.date 格式 DateTimeField 文本字段，值为 datetime.datetime 格式 IntegerField 文本字段，值为整数 DecimalField 文本字段，值为 decimal.Decimal FloatField 文本字段，值为浮点数 BooleanField 复选框，值为 True 和 False RadioField 一组单选框 SelectField 下拉列表 SelectMultipleField 下拉列表，可选择多个值 FileField 文件上传字段 SubmitField 表单提交按钮 FormField 把表单作为字段嵌入另一个表单 FieldList 一组指定类型的字段  验证函数 说　明
Email 验证电子邮件地址 EqualTo 比较两个字段的值；常用于要求输入两次密码进行确认的情况 IPAddress 验证 IPv4 网络地址 Length 验证输入字符串的长度 NumberRange 验证输入的值在数字范围内 Optional 无输入值时跳过其他验证函数 Required 确保字段中有数据 Regexp 使用正则表达式验证输入值 URL 验证 URL AnyOf 确保输入值在可选值列表中 NoneOf 确保输入值不在可选值列表中  </description>
    </item>
    
    <item>
      <title>Mysql 外键约束</title>
      <link>/2017/mysql%E5%A4%96%E9%94%AE%E7%BA%A6%E6%9D%9F/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/mysql%E5%A4%96%E9%94%AE%E7%BA%A6%E6%9D%9F/</guid>
      <description>建表时生成外键
FOREIGN KEY (sid) REFERENCES student (id);
建表后添加外键
ALTER TABLE course_student ADD FOREIGN KEY (sid) REFERENCES student (id);
删除外键
ALTER TABLE course_student DROP FOREIGN KEY course_student_ibfk_1;
外键用于与另一张表的关联。是能确定另一张表记录的字段，用于保持数据的一致性。比如，A表中的一个字段，是B表的主键，那他就可以是A表的外键。
主键、外键和索引的区别
主键 外键 索引
定义 唯一标识一条记录，不能有重复的，不允许为NULL 表的外键是另一表的主键, 外键可以有重复的, 可以是NULL 没有重复值，可以为NULL(会使索引无效)
作用 用来保证数据完整性 用来和其他表建立联系用的 提高查询排序的速度
个数 主键只能有一个 一个表可以有多个外键 一个表可以有多个惟一索引
错误的设计方式—[1215] Cannot add foreign key constraint
出现这种问题的原因一般有两个：
 两张表里要设主键和外键的字段的数据类型或者数据长度不一样。 2.某个表里已经有记录了。  </description>
    </item>
    
    <item>
      <title>Flask的csrf</title>
      <link>/2017/flask%E7%9A%84csrf/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E7%9A%84csrf/</guid>
      <description>CSRF：从一个网站A中发起一个到网站B的请求，而这个请求是经过了伪装的，伪装操作达到的目的就是让请求看起来像是从网站B中发起的，也就是说，让B网站所在的服务器端误以为该请求是从自己网站发起的，而不是从A网站发起的。当然，请求一般都是恶意的。
一、什么是CSRF？
CSRF是Cross Site Request Forgery的缩写，翻译过来就是跨站请求伪造。那么什么是跨站请求伪造呢？让我一个词一个词的解释：
1、跨站：顾名思义，就是从一个网站到另一个网站。
2、请求：即HTTP请求。
3、伪造：在这里可以理解为仿造、伪装。
综合起来的意思就是：从一个网站A中发起一个到网站B的请求，而这个请求是经过了伪装的，伪装操作达到的目的就是让请求看起来像是从网站B中发起的，也就是说，让B网站所在的服务器端误以为该请求是从自己网站发起的，而不是从A网站发起的。当然，请求一般都是恶意的。
看到这里，你可能会问：为什么要伪装成从B网站发起的呢？从网站A直接向B网站服务器发起请求不可以吗？
要回答这个问题，就需要我们对Cookie机制有一定的认识。关于Cookie机制我会单独写一篇文章，这里不做详细介绍。这里你只需要知道：之所以要伪装成从B网站发起的，是因为Cookie是不能跨域发送的。结合上面这个例子来说就是：如果从A网站直接发送请求到B网站服务器的话，是无法将B网站中产生的Cookie一起发给B服务器的。
可能你还会问，为什么非要发送Cookie呢？这是因为服务器在用户登录后会将用户的一些信息放到Cookie中返回给客户端，然后客户端在请求一些需要认证的资源的时候会把Cookie一起发给服务器，服务器通过读取Cookie中的信息来进行用户认证，认证通过后才会做出正确的响应。
A网站访问B网站服务器的一些需要认证的资源的时候，如果没有Cookie信息，服务器是拒绝访问的，那么A网站就无法进行恶意操作。而伪造成B网站的请求，就可以将B网站的Cookie一起发到B服务器，这个时候就服务器就认为该请求是合法的，就会给出正确的响应，这个时候，A网站就达到目的了。
简单一句话就是：攻击者盗用了你的身份，以你的名义发送恶意请求。
那么，A网站通过CSRF能够做那些操作呢？
二、CSRF能够做什么呢？
CSRF能够做的事情包括：以你名义发送邮件，发消息，盗取你的账号，甚至于购买商品，虚拟货币转账……造成的问题包括：个人隐私泄露以及财产安全。等等等等。</description>
    </item>
    
    <item>
      <title>Flask的闪现</title>
      <link>/2017/flask%E7%9A%84%E9%97%AA%E7%8E%B0/</link>
      <pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E7%9A%84%E9%97%AA%E7%8E%B0/</guid>
      <description>Flash中的消息闪现，在官方的解释是用来给用户做出反馈。不过实际上这个功能只是一个记录消息的方法，在某一个请求中记录消息，在下一个请求中获取消息，然后做相应的处理，也就是说flask只存在于两个相邻的请求中“闪现”，第三次请求就不存在这个flash了。
通常情况下配合模板系统进行调用，可以实现一种伪”ajax”请求的效果
基于 flash 模块
from flask import flash
模板输出flash
```
{# 将flash消息闪现和后台联系起来 #}
​ {# 将消息闪现里面的所有消息遍历，取出需要返回给用户的信息 #}
​ {% for message in get_flashed_messages() %}
​ {{ message }}
​ {% endfor %}</description>
    </item>
    
    <item>
      <title>Jinja2模板简介与使用</title>
      <link>/2017/jinja2%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Sat, 15 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/jinja2%E7%AE%80%E4%BB%8B/</guid>
      <description>Jinja2模板引擎简介(template)
模板
视图函数的主要作用是生成请求的响应，这是最简单的请求。实际上，视图函数有两个作用：处理业务逻辑和返回响应内容。在大型应用中，把业务逻辑和表现内容放在一起，会增加代码的复杂度和维护成本。本节学到的模板，它的作用即是承担视图函数的另一个作用，即返回响应内容。
模板其实是一个包含响应文本的文件，其中用占位符(变量)表示动态部分，告诉模板引擎其具体的值需要从使用的数据中获取
使用真实值替换变量，再返回最终得到的字符串，这个过程称为“渲染”
Flask是使用 Jinja2 这个模板引擎来渲染模板
使用模板的好处：
视图函数只负责业务逻辑和数据处理(业务逻辑方面)
而模板则取到视图函数的数据结果进行展示(视图展示方面)
代码结构清晰，耦合度低
Jinja2
两个概念：
Jinja2：是 Python 下一个被广泛应用的模板引擎，是由Python实现的模板语言，他的设计思想来源于 Django 的模板引擎，并扩展了其语法和一系列强大的功能，其是Flask内置的模板语言。
模板语言：是一种被设计来自动生成文档的简单文本格式，在模板语言中，一般都会把一些变量传给模板，替换模板的特定位置上预先定义好的占位变量名。
官方文档
渲染模版函数
Flask提供的 render_template 函数封装了该模板引擎
render_template 函数的第一个参数是模板的文件名，后面的参数都是键值对，表示模板中变量对应的真实值。
1
2
使用
​ {{}} 来表示变量名，这种 {{}} 语法叫做变量代码块
{{ post.title }} Jinja2 模版中的变量代码块可以是任意 Python 类型或者对象，只要它能够被 Python 的 str() 方法转换为一个字符串就可以，比如，可以通过下面的方式显示一个字典或者列表中的某个元素:
{{your_dict[&#39;key&#39;]}} {{your_list[0]}} 用 {%%} 定义的控制代码块，可以实现一些语言层次的功能，比如循环或者if语句 {% if user %} ​ {{ user }} {% else %} ​ hello! &amp;lt;ul&amp;gt; ​ {% for index in indexs %} ​ &amp;lt;li&amp;gt; {{ index }} &amp;lt;/li&amp;gt; ​ {% endfor %} &amp;lt;/ul&amp;gt;  注释</description>
    </item>
    
    <item>
      <title>Flask的flask_script</title>
      <link>/2017/flask%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/</guid>
      <description>flask_script
1、安装：进入到虚拟环境中，pip install flask_script
2、flask_script 作用：可以通过命令行的形式来操作Flask，例如通过命令跑一个开发版本的服务器、设置数据库、定时任务等</description>
    </item>
    
    <item>
      <title>Flask的request</title>
      <link>/2017/flask%E7%9A%84request/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E7%9A%84request/</guid>
      <description>flask的request模块的属性
1、method：请求的方法（get或者post）
2、form：返回form的内容
3、args和values：args返回请求中的参数，values返回请求中的参数和form
4、cookies：cookies信息
5、headers：请求headers信息，返回的结果是个list
6、date、files：date是请求的数据，files随请求上传的文件</description>
    </item>
    
    <item>
      <title>Flask上下文</title>
      <link>/2017/flask%E4%B8%8A%E4%B8%8B%E6%96%87/</link>
      <pubDate>Sat, 10 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E4%B8%8A%E4%B8%8B%E6%96%87/</guid>
      <description>上下文：相当于一个容器，保存了 Flask 程序运行过程中的一些信息。
Flask中有两种上下文，请求上下文和应用上下文
请求上下文(request context)
在 flask 中，可以直接在视图函数中使用 request 这个对象进行获取相关数据，而 request 就是请求上下文的对象，保存了当前本次请求的相关数据，请求上下文对象有：request、session
request
封装了HTTP请求的内容，针对的是http请求。举例：user = request.args.get(&amp;lsquo;user&amp;rsquo;)，获取的是get请求的参数。
session
用来记录请求会话中的信息，针对的是用户信息。举例：session[&amp;lsquo;name&amp;rsquo;] = user.id，可以记录用户信息。还可以通过session.get(&amp;lsquo;name&amp;rsquo;)获取用户信息。
应用上下文(application context)
它的字面意思是 应用上下文，但它不是一直存在的，它只是request context 中的一个对 app 的代理(人)，所谓local proxy。它的作用主要是帮助 request 获取当前的应用，它是伴 request 而生，随 request 而灭的。
应用上下文对象有：current_app，g
current_app
应用程序上下文,用于存储应用程序中的变量，可以通过current_app.name打印当前app的名称，也可以在current_app中存储一些变量，例如：
应用的启动脚本是哪个文件，启动时指定了哪些参数
加载了哪些配置文件，导入了哪些配置
连了哪个数据库
有哪些public的工具类、常量
应用跑再哪个机器上，IP多少，内存多大
current_app.name
current_app.test_value=&amp;lsquo;value&amp;rsquo;
g变量
g 作为 flask 程序全局的一个临时变量,充当者中间媒介的作用,我们可以通过它传递一些数据，g 保存的是当前请求的全局变量，不同的请求会有不同的全局变量，通过不同的thread id区别
g.name=&amp;lsquo;abc&amp;rsquo;
注意：不同的请求，会有不同的全局变量
两者区别：
请求上下文：保存了客户端和服务器交互的数据
应用上下文：flask 应用程序运行过程中，保存的一些配置信息，比如程序名、数据库连接、应用信息等</description>
    </item>
    
    <item>
      <title>docker下搭建FastDFS</title>
      <link>/2017/docker%E4%B8%8B%E6%90%AD%E5%BB%BAfastdfs/</link>
      <pubDate>Mon, 05 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/docker%E4%B8%8B%E6%90%AD%E5%BB%BAfastdfs/</guid>
      <description>利用Docker来搭建分布式文件系统FastDfs ​ 对于文件存储来说，一般情况下简单的处理就是在Django配置文件中配置存储目录，按照规则对文件进行上传或者下载。
​ 实际上，当文件较少的时候，Django是可以应付的过来的。但当文件以海量形式出现的时候，Django就并不是那么好用了，于是Fast DFS应运而出。
​ FastDFS是一个开源的分布式文件系统，它对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等。可以说它就是为互联网而生，为大数据而生的。
​ FastDFS服务端有两个角色：跟踪器（tracker）和存储节点（storage）。跟踪器主要做调度工作，在访问上起负载均衡的作用。 存储节点存储文件，完成文件管理的所有功能：存储、同步和提供存取接口，FastDFS同时对文件的meta data进行管理。跟踪器和存储节点都可以由多台服务器构成。跟踪器和存储节点中的服务器均可以随时增加或下线而不会影响线上服务。其中跟踪器中的所有服务器都是对等的，可以根据服务器的压力情况随时增加或减少。
​
​ 说人话，为啥要用FastDfs:
​ 1 解决海量存储，同时存储容量扩展方便。
​ 2 解决文件内容重复,如果用户上传的文件重复(文件指纹一样)，那么系统只有存储一份数据，值得一提的是，这项技术目前被广泛应用在网盘中。
​ 3 结合Nginx提高网站读取图片的效率。
如果我们从头搭建fastdfs服务器那么就太low了，网上有大把的docker镜像供你选择，所以又到了利用docker优越性的时候了，首先下载fastdfs镜像 1.加载镜像
docker pull delron/fastdfs  ·# docker load -i 文件路径/fastdfs_docker.tar
2. 运行tracker
执行如下命令开启tracker 服务
docker run -d --network=host --name tracker -v /root:/var/root delron/fastdfs tracker  ·# docker run -dti &amp;ndash;network=host &amp;ndash;name tracker -v /var/fdfs/tracker:/var/fdfs delron/fastdfs tracker
我们将fastDFS tracker运行目录映射到本机的/var/root目录中。
执行如下命令查看tracker是否运行起来
docker container ls
如果想停止tracker服务，可以执行如下命令
docker container stop tracker</description>
    </item>
    
    <item>
      <title>Flask的cookie与session</title>
      <link>/2017/flask%E7%9A%84cookie%E4%B8%8Esession/</link>
      <pubDate>Mon, 29 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E7%9A%84cookie%E4%B8%8Esession/</guid>
      <description>Cookie
Cookie：指某些网站为了辨别用户身份、进行会话跟踪而储存在用户本地的数据（通常经过加密）。
复数形式Cookies。
Cookie最早是网景公司的前雇员Lou Montulli在1993年3月的发明。
Cookie是由服务器端生成，发送给客户端浏览器，浏览器会将Cookie的key/value保存，下次请求同一网站时就发送该Cookie给服务器（前提是浏览器设置为启用cookie）。
Cookie的key/value可以由服务器端自己定义。
应用：
最典型的应用是判定注册用户是否已经登录网站，用户可能会得到提示，是否在下一次进入此网站时保留用户信息以便简化登录手续，这些都是Cookie的功用。
网站的广告推送，经常遇到访问某个网站时，会弹出小窗口，展示我们曾经在购物网站上看过的商品信息。
购物车，用户可能会在一段时间内在同一家网站的不同页面中选择不同的商品，这些信息都会写入Cookie，以便在最后付款时提取信息。
提示：
Cookie是存储在浏览器中的一段纯文本信息，建议不要存储敏感信息如密码，因为电脑上的浏览器可能被其它人使用
Cookie基于域名安全，不同域名的Cookie是不能互相访问的
如访问itcast.cn时向浏览器中写了Cookie信息，使用同一浏览器访问baidu.com时，无法访问到itcast.cn写的Cookie信息
浏览器的同源策略
当浏览器请求某网站时，会将本网站下所有Cookie信息提交给服务器，所以在request中可以读取Cookie信息
设置cookie
from flask imoprt Flask,make_response
@app.route(&amp;lsquo;/cookie&amp;rsquo;)
def set_cookie():
​ resp = make_response(&amp;lsquo;this is to set cookie&amp;rsquo;)
​ resp.set_cookie(&amp;lsquo;username&amp;rsquo;, &amp;lsquo;itcast&amp;rsquo;)
​ return resp
设置过期时间
@app.route(&amp;lsquo;/cookie&amp;rsquo;)
def set_cookie():
​ response = make_response(&amp;lsquo;hello world&amp;rsquo;)
​ response.set_cookie(&amp;lsquo;username&amp;rsquo;, &amp;lsquo;itheima&amp;rsquo;, max_age=3600)
​ return response
获取cookie
from flask import Flask,request
#获取cookie
@app.route(&amp;lsquo;/request&amp;rsquo;)
def resp_cookie():
​ resp = request.cookies.get(&amp;lsquo;username&amp;rsquo;)
return resp</description>
    </item>
    
    <item>
      <title>Flask两大核心</title>
      <link>/2017/flask%E4%B8%A4%E5%A4%A7%E6%A0%B8%E5%BF%83/</link>
      <pubDate>Sat, 13 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E4%B8%A4%E5%A4%A7%E6%A0%B8%E5%BF%83/</guid>
      <description>Flask有两大核心：Werkzeug和Jinja2
- Werkzeug实现路由、调试和Web服务器网关接口
- Jinja2实现了模板。
Werkzeug是一个遵循WSGI协议的python函数库
- 其内部实现了很多Web框架底层的东西，比如request和response对象；
- 与WSGI规范的兼容；支持Unicode；
- 支持基本的会话管理和签名Cookie；
- 集成URL请求路由等。
Werkzeug库的 routing 模块负责实现 URL 解析。不同的 URL 对应不同的视图函数，routing模块会对请求信息的URL进行解析，匹配到URL对应的视图函数，执行该函数以此生成一个响应信息。
routing模块内部有：
Rule类
用来构造不同的URL模式的对象，路由URL规则
Map类
存储所有的URL规则和一些配置参数
BaseConverter的子类
负责定义匹配规则
MapAdapter类
负责协调Rule做具体的匹配的工作
简而言之 Werkzeug实现了路由功能 Jinja2实现了模板功能</description>
    </item>
    
    <item>
      <title>Flask简述与安装</title>
      <link>/2017/flask%E7%AE%80%E8%BF%B0%E4%B8%8E%E5%AE%89%E8%A3%85/</link>
      <pubDate>Tue, 02 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/flask%E7%AE%80%E8%BF%B0%E4%B8%8E%E5%AE%89%E8%A3%85/</guid>
      <description>Flask
Flask诞生于2010年，是Armin ronacher（人名）用 Python 语言基于 Werkzeug 工具箱编写的轻量级Web开发框架。
Flask 本身相当于一个内核，其他几乎所有的功能都要用到扩展（邮件扩展Flask-Mail，用户认证Flask-Login，数据库Flask-SQLAlchemy），都需要用第三方的扩展来实现。比如可以用 Flask 扩展加入ORM、窗体验证工具，文件上传、身份验证等。Flask 没有默认使用的数据库，你可以选择 MySQL，也可以用 NoSQL。
其 WSGI 工具箱采用 Werkzeug（路由模块），模板引擎则使用 Jinja2。这两个也是 Flask 框架的核心。
Flask 是python三大web框架之一，比之其他两大框架django和tornado而言，它更加的轻量化，开发效率更高，是入门web框架的首选。
同类的python框架还有 bottle web.py 等
Flask常用扩展包：
Flask-SQLalchemy：操作数据库；
Flask-script：插入脚本；
Flask-migrate：管理迁移数据库；
Flask-Session：Session存储方式指定；
Flask-WTF：表单；
Flask-Mail：邮件；
Flask-Bable：提供国际化和本地化支持，翻译；
Flask-Login：认证用户状态；
Flask-OpenID：认证；
Flask-RESTful：开发REST API的工具；
Flask-Bootstrap：集成前端Twitter Bootstrap框架；
Flask-Moment：本地化日期和时间；
Flask-Admin：简单而可扩展的管理接口的框架
安装flask
pip install Flask
列出pip已经安装好的模块
pip freeze
三种导入配置文件的方式：
配置对象，里面定义需要给 APP 添加的一系列配置 class Config(object):
DEBUG = True
从配置对象中加载配置 app.config.from_object(Config)
从配置文件中加载配置 app.config.from_pyfile(&amp;lsquo;config.ini&amp;rsquo;)
#由环境变量里面来加载配置
app.config.from_envvar(&amp;lsquo;app_config&amp;rsquo;)
路由配置，指定methods方法，返回json数据
@app.route(&amp;lsquo;/demo4&amp;rsquo;,methods=[&amp;lsquo;GET&amp;rsquo;, &amp;lsquo;POST&amp;rsquo;])
def demo4():</description>
    </item>
    
    <item>
      <title>redis</title>
      <link>/2017/redis/</link>
      <pubDate>Sun, 26 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/redis/</guid>
      <description> Redis持久化是如何工作的？ 简单来说就是将数据放到断电后数据不会丢失的设备中，也就是我们通常理解的硬盘。
首先我们来看一下数据库在进行写操作时到底做了哪些事，主要有下面五个过程：
 客户端向服务端发送写操作（数据在客户端的内存中）。 数据库服务端接收到写请求的数据（数据在服务端的内存中）。 服务端调用write这个系统调用，将数据往磁盘上写（数据在系统内存的缓冲区中）。 操作系统将缓冲区中的数据转移到磁盘控制器上（数据在磁盘缓存中）。 磁盘控制器将数据写到磁盘的物理介质中（数据真正落到磁盘上）。  这个是过程。
 当数据库系统故障时，这时候系统内核还是完好的。那么此时只要我们执行完了第3步，那么数据就是安全的，因为后续操作系统会来完成后面几步，保证数据最终会落到磁盘上。 当系统断电时，这时候上面5项中提到的所有缓存都会失效，并且数据库和操作系统都会停止工作。所以只有当数据在完成第5步后，才能保证在断电后数据不丢失。  通过上面5步的了解，可能我们会希望搞清下面一些问题：
 数据库多长时间调用一次write，将数据写到内核缓冲区？ 内核多长时间会将系统缓冲区中的数据写到磁盘控制器？ 磁盘控制器又在什么时候把缓存中的数据写到物理介质上？  对于第一个问题，通常数据库层面会进行全面控制。而对第二个问题，操作系统有其默认的策略，但是我们也可以通过POSIX API提供的fsync系列命令强制操作系统将数据从内核区写到磁盘控制器上。对于第三个问题，好像数据库已经无法触及，但实际上，大多数情况下磁盘缓存是被设置关闭的，或者是只开启为读缓存，也就是说写操作不会进行缓存，直接写到磁盘。建议的做法是仅仅当你的磁盘设备有备用电池时才开启写缓存。
数据损坏 所谓数据损坏，就是数据无法恢复，上面我们讲的都是如何保证数据是确实写到磁盘上去，但是写到磁盘上可能并不意味着数据不会损坏。比如我们可能一次写请求会进行两次不同的写操作，当意外发生时，可能会导致一次写操作安全完成，但是另一次还没有进行。如果数据库的数据文件结构组织不合理，可能就会导致数据完全不能恢复的状况出现。
这里通常也有三种策略来组织数据，以防止数据文件损坏到无法恢复的情况：
 第一种是最粗糙的处理，就是不通过数据的组织形式保证数据的可恢复性。而是通过配置数据同步备份的方式，在数据文件损坏后通过数据备份来进行恢复。实际上MongoDB在不开启操作日志，通过配置Replica Sets时就是这种情况。 另一种是在上面基础上添加一个操作日志，每次操作时记一下操作的行为，这样我们可以通过操作日志来进行数据恢复。因为操作日志是顺序追加的方式写的，所以不会出现操作日志也无法恢复的情况。这也类似于MongoDB开启了操作日志的情况。 更保险的做法是数据库不进行旧数据的修改，只是以追加方式去完成写操作，这样数据本身就是一份日志，这样就永远不会出现数据无法恢复的情况了。实际上CouchDB就是此做法的优秀范例。  </description>
    </item>
    
    <item>
      <title>Vue简述</title>
      <link>/2017/vue%E7%AE%80%E8%BF%B0/</link>
      <pubDate>Tue, 10 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/vue%E7%AE%80%E8%BF%B0/</guid>
      <description>Vue (读音 /vjuː/，类似于 view) 是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具以及各种类库结合使用时，Vue 也完全能够为复杂的单页应用提供驱动。
Vue.js 使用了基于 HTML 的模板语法，允许开发者声明式地将 DOM 绑定至底层 Vue 实例的数据。所有 Vue.js 的模板都是合法的 HTML ，所以能被遵循规范的浏览器和 HTML 解析器解析。
在底层的实现上，Vue 将模板编译成虚拟 DOM 渲染函数。结合响应系统，Vue 能够智能地计算出最少需要重新渲染多少组件，并把 DOM 操作次数减到最少。
如果你熟悉虚拟 DOM 并且偏爱 JavaScript 的原始力量，你也可以不用模板，直接写渲染 (render)，使用可选的 JSX 语法。
vue自动化流程安装步骤：
安装cnpm
npm install -g cnpm &amp;ndash;registry=https://registry.npm.taobao.org
安装 vue-cli
cnpm install -g vue-cli
清空缓存
npm cache clean —force
建立项目
vue init webpack nvue(项目名称)
进入项目目录
cd nvue
热启动服务
npm run dev
如果想停止服务
ctrl + c</description>
    </item>
    
    <item>
      <title>Docker特点</title>
      <link>/2016/docker%E7%89%B9%E7%82%B9/</link>
      <pubDate>Sun, 11 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/docker%E7%89%B9%E7%82%B9/</guid>
      <description> Docker特点 1.上手快
用户只需要几分钟，就可以把自己的程序“Docker 化”。Docker 依赖于“写时复制” (copy-on-write)模型，使修改应用程序也非常迅速，可以说达到“随心所致，代码即改” 的境界。
随后，就可以创建容器来运行应用程序了。大多数 Docker 容器只需要不到 1 秒中即可 启动。由于去除了管理程序的开销，Docker 容器拥有很高的性能，同时同一台宿主机中也 可以运行更多的容器，使用户尽可能的充分利用系统资源。
2.职责的逻辑分类
使用 Docker，开发人员只需要关心容器中运行的应用程序，而运维人员只需要关心如 何管理容器。Docker 设计的目的就是要加强开发人员写代码的开发环境与应用程序要部署 的生产环境一致性。从而降低那种“开发时一切正常，肯定是运维的问题(测试环境都是正 常的，上线后出了问题就归结为肯定是运维的问题)”
3.快速高效的开发生命周期
Docker 的目标之一就是缩短代码从开发、测试到部署、上线运行的周期，让你的应用 程序具备可移植性，易于构建，并易于协作。(通俗一点说，Docker 就像一个盒子，里面 可以装很多物件，如果需要这些物件的可以直接将该大盒子拿走，而不需要从该盒子中一件 件的取。)
4.鼓励使用面向服务的架构
Docker 还鼓励面向服务的体系结构和微服务架构。Docker 推荐单个容器只运行一个应 用程序或进程，这样就形成了一个分布式的应用程序模型，在这种模型下，应用程序或者服 务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序 都变得非常简单，同时也提高了程序的内省性。(当然，可以在一个容器中运行多个应用程 序)
使用Docker做什么 容器提供了隔离性，容器可以为各种测试提供很好的沙盒环境。并且，容器本
身就具有“标准性”的特征，非常适合为服务创建构建块。Docker 的一些应用场景如下:
 加速本地开发和构建流程，使其更加高效、更加轻量化。本地开发人员可以构建、 运行并分享 Docker 容器。容器可以在开发环境中构建，然后轻松的提交到测试环境中，并 最终进入生产环境。 能够让独立的服务或应用程序在不同的环境中，得到相同的运行结果。这一点在 面向服务的架构和重度依赖微型服务的部署由其实用。 用 Docker 创建隔离的环境来进行测试。例如，用 Jenkins CI 这样的持续集成工具 启动一个用于测试的容器。 Docker 可以让开发者先在本机上构建一个复杂的程序或架构来进行测试，而不是 一开始就在生产环境部署、测试。  </description>
    </item>
    
    <item>
      <title>挂起我的服务器screen</title>
      <link>/2016/%E6%8C%82%E8%B5%B7%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8screen/</link>
      <pubDate>Fri, 21 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/%E6%8C%82%E8%B5%B7%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8screen/</guid>
      <description>你是不是经常需要 SSH 或者 telent 远程登录到 Linux 服务器？你是不是经常为一些长时间运行的任务而头疼，比如系统备份、ftp 传输等等。通常情况下我们都是为每一个这样的任务开一个远程终端窗口，因为他们执行的时间太长了。必须等待它执行完毕，在此期间可不能关掉窗口或者断开连接，否则这个任务就会被杀掉，一切半途而废了。那么究竟是什么到导致的呢？
==元凶：SIGHUP 信号== 让我们来看看为什么关掉窗口/断开连接会使得正在运行的程序死掉。
在Linux/Unix中，有这样几个概念：
 进程组（process group）：一个或多个进程的集合，每一个进程组有唯一一个进程组ID，即进程组长进程的ID。 会话期（session）：一个或多个进程组的集合，有唯一一个会话期首进程（session leader）。会话期ID为首进程的ID。 会话期可以有一个单独的控制终端（controlling terminal）。与控制终端连接的会话期首进程叫做控制进程（controlling process）。当前与终端交互的进程称为前台进程组。其余进程组称为后台进程组。  根据POSIX.1定义：
 挂断信号（SIGHUP）默认的动作是终止程序。 当终端接口检测到网络连接断开，将挂断信号发送给控制进程（会话期首进程）。 如果会话期首进程终止，则该信号发送到该会话期前台进程组。 一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。  因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。
我们来看一个例子。打开两个SSH终端窗口，在其中一个运行top命令。
`[root@tivf09 root]# top`  在另一个终端窗口，找到top的进程ID为5180，其父进程ID为5128，即登录shell。
`[root@tivf09 root]# ps -ef|grep top``root 5180 5128 0 01:03 pts/0 00:00:02 top``root 5857 3672 0 01:12 pts/2 00:00:00 grep top`  使用pstree命令可以更清楚地看到这个关系：
`[root@tivf09 root]# pstree -H 5180|grep top``|-sshd-+-sshd---bash---top`  使用ps-xj命令可以看到，登录shell（PID 5128）和top在同一个会话期，shell为会话期首进程，所在进程组PGID为5128，top所在进程组PGID为5180，为前台进程组。
`[root@tivf09 root]# ps -xj|grep 5128`` ``5126 5128 5128 5128 pts/0 5180 S 0 0:00 -bash`` ``5128 5180 5180 5128 pts/0 5180 S 0 0:50 top`` ``3672 18095 18094 3672 pts/2 18094 S 0 0:00 grep 5128`  关闭第一个SSH窗口，在另一个窗口中可以看到top也被杀掉了。</description>
    </item>
    
    <item>
      <title>排序算法</title>
      <link>/2016/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</guid>
      <description>  Python中排序分为：  插入排序： 直接插入排序 希尔排序 选择排序： 简单选择排序 堆排序 交换排序： 快速排序 冒泡排序 归并排序 基数排序 </description>
    </item>
    
    <item>
      <title>mysql</title>
      <link>/2016/mysql/</link>
      <pubDate>Tue, 18 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/mysql/</guid>
      <description>数据库的使用，是开发人员的基本功，对它掌握越清晰越深入，你能做的事情就越多。
 做业务，要懂基本的SQL语句； 做性能优化，要懂索引，懂引擎； 做分库分表，要懂主从，懂读写分离&amp;hellip;
 今天我们用10分钟，重点梳理一遍以下几方面：
 数据库知识点汇总； 数据库事务特性和隔离级别； 详解关系型数据库、索引与锁机制； 数据库调优与最佳实践； 面试考察点及加分项。  知识点汇总
一、数据库的不同类型
1.常用的关系型数据库
 Oracle：功能强大，主要缺点就是贵 MySQL：互联网行业中最流行的数据库，这不仅仅是因为MySQL的免费。可以说关系数据库场景中你需要的功能，MySQL都能很好的满足，后面详解部分会详细介绍MySQL的一些知识点 MariaDB：是MySQL的分支，由开源社区维护，MariaDB虽然被看作MySQL的替代品，但它在扩展功能、存储引擎上都有非常好的改进 PostgreSQL：也叫PGSQL，PGSQL类似于Oracle的多进程框架，可以支持高并发的应用场景，PG几乎支持所有的SQL标准，支持类型相当丰富。PG更加适合严格的企业应用场景，而MySQL更适合业务逻辑相对简单、数据可靠性要求较低的互联网场景。  2.NoSQL数据库（非关系型数据库）
 Redis：提供了持久化能力，支持多种数据类型。Redis适用于数据变化快且数据大小可预测的场景。 MongoDB：一个基于分布式文件存储的数据库，将数据存储为一个文档，数据结构由键值对组成。MongoDB比较适合表结构不明确，且数据结构可能不断变化的场景，不适合有事务和复杂查询的场景。 HBase：建立在HDFS，也就是Hadoop文件系统之上的分布式面向列的数据库。类似于谷歌的大表设计，HBase可以提供快速随机访问海量结构化数据。在表中它由行排序，一个表有多个列族以及每一个列族可以有任意数量的列。 HBase依赖HDFS可以实现海量数据的可靠存储，适用于数据量大，写多读少，不需要复杂查询的场景。 Cassandra：一个高可靠的大规模分布式存储系统。支持分布式的结构化Key-value存储，以高可用性为主要目标。适合写多的场景，适合做一些简单查询，不适合用来做数据分析统计。 Pika：一个可持久化的大容量类Redis存储服务， 兼容五种主要数据结构的大部分命令。Pika使用磁盘存储，主要解决Redis大容量存储的成本问题。  3.NewSQL数据库（新一代关系型数据库）
 TiDB：开源的分布式关系数据库，几乎完全兼容MySQL，能够支持水平弹性扩展、ACID事务、标准SQL、MySQL语法和MySQL协议，具有数据强一致的高可用特性。既适合在线事务处理，也适合在线分析处理。 OceanBase：OceanBase是蚂蚁金服的数据库，OB是可以满足金融级的可靠性和数据一致性要求的数据库系统。当你需要使用事务，并且数据量比较大，就比较适合使用OB。不过目前OB已经商业化，不再开源。  二、事物特性及事物类型
后面的详解知识点会展开介绍
三、数据库的范式
前关系数据库有六种范式：第一范式、第二范式、第三范式、巴斯-科德范式（BCNF）、第四范式和第五范式。范式级别越高对数据表的要求越严格。
 第一范式要求最低，只要求表中字段不可用在拆分。 第二范式在第一范式的基础上要求每条记录由主键唯一区分，记录中所有属性都依赖于主键。 第三范式在第二范式的基础上，要求所有属性必须直接依赖主键，不允许间接依赖。 一般说来，数据库只需满足第三范式就可以了。  详解知识点一：数据库事务
知识点
▌1.数据库事务特性
数据库的特性是面试时考察频率非常高的题目，共4个特性：
 原子性：是指事务由原子的操作序列组成，所有操作要么全部成功，要么全部失败回滚。 一致性：是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处以一致性状态。比如在做多表操作时，多个表要么都是事务后新的值，要么都是事务前的旧值。 隔离性：是指多个用户并发访问数据库时，数据库为每个用户执行的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。事务的隔离级别我们稍后介绍。 持久性：是指一个事务一旦提交并执行成功，那么对数据库中数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。  ▌2.事物并发问题与隔离级别
a.事务并发问题
 脏读：脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据，例如，账户A转帐给B500元，B余额增加后但事务还没有提交完成，此时如果另外的请求中获取的是B增加后的余额，这就发生了脏读，因为事务如果失败回滚时，B的余额就不应该增加。 不可重复读：不可重复读是指对于数据库中某个数据，一个事务范围内多次查询返回了不同的数据值，这是由于在多次查询之间，有其他事务修改了数据并进行了提交。 幻读：是指一个事务中执行两次完全相同的查询时，第二次查询所返回的结果集跟第一个查询不相同。与不可重复读的区别在于，不可重复读是对同一条记录，两次读取的值不同。而幻读是记录的增加或删除，导致两次相同条件获取的结果记录数不同。  b：事务的四种隔离级别
可以用于解决这几种并发问题。如图右面，由上到下的4种隔离级别由低到高。
 级别1读未提交：也就是可以读取到其他事务未提交的内容，这是最低的隔离级别，这个隔离级别下，前面提到的三种并发问题都有可能发生。 级别2读已提交：就是只能读取到其他事务已经提交的数据。这个隔离级别可以解决脏读问题。 级别三可重复读：可以保证整个事务过程中，对同数据的多次读取结果是相同的。这个级别可以解决脏读和不可重复读的问题。MySQL默认的隔离级别就是可重复读。 级别四串行化：这是最高的隔离级别，所有事务操作都依次顺序执行。这个级别会导致并发度下降，性能最差。不过这个级别可以解决前面提到的所有并发问题。  ▌3.</description>
    </item>
    
    <item>
      <title>Mysql 视图（view）</title>
      <link>/2016/mysql%E8%A7%86%E5%9B%BE/</link>
      <pubDate>Thu, 13 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/mysql%E8%A7%86%E5%9B%BE/</guid>
      <description>什么是视图
通俗的讲，视图就是一条SELECT语句执行后返回的结果集。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。
视图的特性
视图是对若干张基本表的引用，一张虚表，查询语句执行的结果，不存储具体的数据（基本表数据发生了改变，视图也会跟着改变）；
可以跟基本表一样，进行增删改查操作(ps:增删改操作有条件限制)；
视图的作用
方便操作，特别是查询操作，减少复杂的SQL语句，增强可读性；
更加安全，数据库授权命令不能限定到特定行和特定列，但是通过合理创建视图，可以把权限限定到行列级别；
使用场合
权限控制的时候，不希望用户访问表中某些含敏感信息的列，比如salary&amp;hellip;
关键信息来源于多个复杂关联表，可以创建视图提取我们需要的信息，简化操作；
建立视图
建议名称以v_开头，用来和普通表区分
使用show tables 可以显示视图
create view 视图名称 as select语句;
使用视图
select * from 视图名称
删除视图
drop view 视图名称
修改视图
create or replace view 视图名称 as sql语句</description>
    </item>
    
    <item>
      <title>脏读</title>
      <link>/2016/%E8%84%8F%E8%AF%BB/</link>
      <pubDate>Thu, 22 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/%E8%84%8F%E8%AF%BB/</guid>
      <description>脏读 ​ 脏读又称无效数据的读出，是指在数据库访问中，事务T1将某一值修改，然后事务T2读取该值，此后T1因为某种原因撤销对该值的修改，这就导致了T2所读取到的数据是无效的。
剪短点讲：脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是脏数据，依据脏数据所做的操作可能是不正确的。</description>
    </item>
    
    <item>
      <title>bi架构</title>
      <link>/1/bi%E6%9E%B6%E6%9E%84%E6%A6%82%E5%BF%B5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/bi%E6%9E%B6%E6%9E%84%E6%A6%82%E5%BF%B5/</guid>
      <description>BI是什么？BI(Business Intelligence)的中文译名是商务智能，关于这个名词的定义很多，比较严谨的定义如下：
　“商务智能是企业利用现代信息技术收集、管理和分析结构化和非结构化的商务数据和信息，创造和累计商务知识和见解，改善商务决策水平，采取有效的商务行动，完善各种商务流程，提升各方面商务绩效，增强综合竞争力的智慧和能力。”（作者：王茁）
　也有比较简洁的定义：商务智能好比“数据炼油厂”，即把商业活动中累积的数据加工成可用于支持商业决策的信息。
　BI是如何产生的？这需要从传统的商务交易系统讲起。
　最初在商务交易中引入计算机辅助管理时，开发人员是根据企业已规定好的业务规则来编写交易系统。此时的商务系统，其主要目的是让“商务流程自动化”，从而缩短业务周期，提高效率，增强企业的竞争力，最终为企业创造更大的利润。现今，绝大部分大、中型商业公司都已在内部或多或少的引入的计算机辅助商务管理系统。
　随着计算机在商业管理中的普及，公司的高层管理人员有了更近一步的需求，即其企业的部门框架和业务规则随着社会分工的日益细化，而不断的发生变动。而且，其中蕴含了不少的新的商机，精明的经理们当然不希望错过这些能让企业更上层楼的机会了，而原有的商务管理系统面对日益变化的业务规则逐渐变得力不从心。
　因此，软件厂商针对新出现的商业部门和业务规则，推出了一系列的自成体系的，专门针对某块商业数据管理的管理软件，如财务管理软件，客户关系管理软件，产品数据管理软件，人力资源管理软件等。但是，这些自成体系的的管理软件之间，数据很难共享，从而在企业各个部门之间形成了“信息孤立”的局面。
　于是，软件厂商又推出了更大块集成的企业资源规划(ERP)系统，把之前推出的各块独立的管理系统整合起来。但是，单单把各个商务部门的管理软件集成起来，是否真的就是企业真正需要的“能适应商务变化”的整体解决方案呢？
　我认为：如果仅仅针对目前的商务活动和业务规则打包，答案一定是NO! 这个答案也早就被相关方面的专家所确定。那么，如何才能真正把各个商业部门之间的商务数据集成起来，从中预测商务变化，找到潜在商机，为商业决策提供数据支持呢？答案就是BI。
　不过，BI的范围太广太大，在实际商务中我们往往只需运用其中的某个部分就可以暂时满足企业的需求，如数据仓库，联机事务分析(OLAP)，数据挖掘，决策支持系统(DDS)等。其实，整个BI的框架结构可以用下面的图中间的三部分(数据预处理、数据仓库、数据分析)来表示：
数据分析：OLAP和数据挖掘
　OLAP与数据挖掘是一个有机的整体，在OLAP中必定要针对不同的主题数据仓库采用相应的数据挖掘算法来进行数据分析。如果把数据仓库对BI系统的作用比作厨师的食材，那么，OLAP和数据挖掘则是厨具。
　联机分析处理(OLAP)的概念最早是由关系数据库之父E.F.Codd于1993年提出的，其目的是为了让管理者灵活地对海量数据进行浏览分析。当时,Codd认为联机事务处理(OLTP)已不能满足终端用户对数据库查询分析的需要,SQL对大数据库进行的简单查询也不能满足用户分析的需求。用户的决策分析需要对关系数据库进行大量计算才能得到结果,而查询的结果并不能满足决策者提出的需求。因此Codd提出了多维数据库和多维分析的概念,即OLAP。Codd提出OLAP的12条准则来描述OLAP系统： 准则1　OLAP模型必须提供多维概念视图 准则2　透明性准则 准则3　存取能力推测 准则4　稳定的报表能力 准则5　客户/服务器体系结构 准则6　维的等同性准则 准则7　动态的稀疏矩阵处理准则 准则8　多用户支持能力准则 准则9　非受限的跨维操作 准则10 直观的数据操纵 准则11 灵活的报表生成 准则12 不受限的维与聚集层次
　　利用多维的概念，OLAP提供了切片、切块、下钻、上卷和旋转等多维度分析与跨维度分析功能。相对于普通的静态报表，OLAP更能满足决策者和分析人员对数据仓库数据的分析。OLAP系统架构主要分为基于关系数据库的ROLAP（Relational OLAP）、基于多维数据库的MOLAP（Multidimensional OLAP）、基于混合数据组织的HOLAP（Hybrid OLAP）三种。前两种方式比较常见。ROLAP表示基于关系数据库的OLAP实现。它以关系数据库为核心，以关系型结构进行多维数据的表示和存储。ROLAP将多维数据库的多维结构划分为两类表:一类是事实表，用来存储数据和维关键字；另一类是维表，即对每个维至少使用一个表来存放维的层次、成员类别等维的描述信息。MOLAP表示基于多维数据组织的OLAP实现。它以多维数据组织方式为核心，使用多维数组存储数据。MOLAP查询方式采用索引搜索与直接寻址相结合的方式，比ROLAP的表索引搜索和表连接方式速度要快得多。 数据挖掘（Data Mining，DM）是指从大量不完全的、有噪声的、模糊的、随机的数据中，提取隐含在其中的、有用的信息和知识的过程。其表现形式为概念（Concepts）、规则(Rules)、模式(Patterns)等形式。
　从商业层来看，我个人认为，在商业智能系统中进行数据挖掘的目标大致可分为两类： ①从累积的业务数据中发掘出管理层事先不知道的、但又是潜在有用的信息，为其创造新的商业机会。商业销售已有大量这方面的运用实例，BI业内流传已久的“啤酒和尿布”，以及我在本文开头所举的例子就属此类。 ②从累积的业务数据中寻求最优的资源规划方案，降低成本，从而提高利润。让我们先从大家可能都想过一个例子谈起——邮递员送信，假设我是某个城市的邮递员，一次要送出多封信件，收信人的住址分布在城市的各个街道上。那么该如何设计线路，来尽可能的减少行程呢？商业活动中出现大量类似的例子，当可供分析的数据不多时，我们可以用纸笔来手工计算，找到最优解。但是，如果原始数据量极为庞大的话，我们将不得不求助于计算机了。
　目前业内已有很多成熟的数据挖掘方法论，为实际应用提供了理想的指导模型。CRISP-DM（Cross-Industry Standard Process for Data Mining）就是公认的、较有影响的方法论之一。CRISP-DM强调，DM不单是数据的组织或者呈现，也不仅是数据分析和统计建模，而是一个从理解业务需求、寻求解决方案到接受实践检验的完整过程。CRISP-DM将整个挖掘过程分为以下六个阶段：商业理解（Business Understanding），数据理解(Data Understanding)，数据准备(Data Preparation)，建模(Modeling)，评估(Evaluation)和发布(Deployment)。其框架图如下：</description>
    </item>
    
    <item>
      <title>pywin32控制你的打印机</title>
      <link>/1/pywin32%E6%8E%A7%E5%88%B6%E4%BD%A0%E7%9A%84%E6%89%93%E5%8D%B0%E6%9C%BA/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/pywin32%E6%8E%A7%E5%88%B6%E4%BD%A0%E7%9A%84%E6%89%93%E5%8D%B0%E6%9C%BA/</guid>
      <description>pywin32控制你的打印机
` def printer(name,phone,place): import win32ui import win32gui import win32print import win32con print_name =win32print.GetDefaultPrinter() # test 获取默认打印机 pHandle = win32print.OpenPrinter(win32print.GetDefaultPrinter()) printinfo = win32print.GetPrinter(pHandle,2) pDevModeObj = printinfo[&amp;quot;pDevMode&amp;quot;] pDevModeObj.Scale=10 # pDevModeObj.Orientation=win32con.DMORIENT_LANDSCAPE # 文本横向 print(pDevModeObj.Scale) DC=win32gui.CreateDC(&#39;WINSPOOL&#39;,print_name,pDevModeObj) hDC=win32ui.CreateDCFromHandle(DC) hDC.StartDoc(&amp;quot;Test doc&amp;quot;) hDC.StartPage() # hDC.SetMapMode(win32con.MM_TWIPS*50) hDC.TextOut(20,10,&amp;quot;我是要打印的内容&amp;quot;) #draws text within a box (assume about 1400 dots per inch for typical HP printer) ulc_x = 100 # give a left margin ulc_y = -100 # give a top margin lrc_x = 160 # width of text area-margin, close to right edge of page lrc_y = -110 # height of text area-margin, close to bottom of the page hDC.</description>
    </item>
    
  </channel>
</rss>